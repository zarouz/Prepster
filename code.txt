
GOOGLE_API_KEY=AIzaSyDs_hueG_0BWS4itW3dQ6ATxO9vAfHYuKc
GOOGLE_CLOUD_PROJECT_ID="unrealengine-454118"
GOOGLE_APPLICATION_CREDENTIALS="/Users/karthikyadav/Desktop/realtimeProjects/google cloud/tts:stt/ai-interview-simulator-key.json"
#cline
openrouter = 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



# Flask Core
FLASK_SECRET_KEY=your_strong_random_secret_key_here # Generate a strong random key
FLASK_DEBUG=Talse # Set to True for development debugging

# Security Salts
SECURITY_PASSWORD_SALT=your_strong_random_password_salt_here # Another random key
EMAIL_CONFIRMATION_SALT=your_strong_random_email_salt_here # Another random key

# SQLAlchemy Database (Users, Reports, etc.)
SQLALCHEMY_DB_USER="karthikyadav" 
SQLALCHEMY_DB_PASSWORD= ""
SQLALCHEMY_DB_NAME="users"
SQLALCHEMY_DB_HOST=localhost
SQLALCHEMY_DB_PORT= "5432"

# RAG Database (Knowledge Base)
RAG_ENABLED=True # Set to False to disable RAG features entirely
RAG_DB_USER= "karthikyadav" 
RAG_DB_PASSWORD= ""
RAG_DB_NAME="KnowledgeBase"
RAG_DB_HOST=localhost
RAG_DB_PORT= "5432"

# Google
GOOGLE_API_KEY=
GOOGLE_CLOUD_PROJECT_ID=
GOOGLE_APPLICATION_CREDENTIALS=

# Mail (Example for Gmail - Use App Password if 2FA is enabled)
MAIL_SERVER=smtp.googlemail.com
MAIL_PORT=587
MAIL_USE_TLS=true
MAIL_USERNAME=your_email_address@gmail.com
MAIL_PASSWORD=your_gmail_app_password # Or regular password if less secure apps allowed
MAIL_DEFAULT_SENDER=your_email_address@gmail.com

# --- External Service Endpoints ---
EMOTION_API_ENDPOINT = os.environ.get("EMOTION_API_ENDPOINT", "http://127.0.0.1:5003/analyze")
NEUROSYNC_PLAYER_HOST="127.0.0.1"
NEUROSYNC_PLAYER_PORT="5678"


refactor the entire codebase based on the above .env file.

# modules/audio_utils.py
import os
import io
import warnings
import logging
import soundfile as sf # Still needed to check sample rate of uploaded file

# Local configuration import
import config

# Google Cloud STT
try:
    from google.cloud import speech as google_speech
    GOOGLE_CLOUD_AVAILABLE = True
except ImportError:
    GOOGLE_CLOUD_AVAILABLE = False
    google_speech = None
    logging.getLogger(__name__).warning("Google Cloud Speech library not found (google-cloud-speech). STT features disabled.")

logger = logging.getLogger(__name__)

# --- Global STT Client ---
STT_CLIENT = None

# --- Initialization Function ---
def initialize_stt_client():
    """Initializes Google Cloud STT client if available."""
    global STT_CLIENT, GOOGLE_CLOUD_AVAILABLE

    if not GOOGLE_CLOUD_AVAILABLE:
        logger.warning("Google Cloud library not available. Cannot initialize STT client.")
        return False

    if STT_CLIENT is None:
        logger.info("Initializing Google STT Client...")
        if not config.GOOGLE_CLOUD_PROJECT_ID:
             logger.error("GOOGLE_CLOUD_PROJECT_ID not configured. Cannot initialize STT client.")
             STT_CLIENT = None
             GOOGLE_CLOUD_AVAILABLE = False # Mark as unavailable if project ID missing
             return False
        try:
            # Could pass credentials explicitly if needed:
            # from google.oauth2 import service_account
            # credentials = service_account.Credentials.from_service_account_file('path/to/keyfile.json')
            # STT_CLIENT = google_speech.SpeechClient(credentials=credentials)
            STT_CLIENT = google_speech.SpeechClient()
            # Test connection (optional, adds latency but confirms setup)
            # try:
            #     STT_CLIENT.list_recognizers(parent=f"projects/{config.GOOGLE_CLOUD_PROJECT_ID}/locations/global") # Example lightweight call
            #     logger.info("STT client connection test successful.")
            # except Exception as test_err:
            #      logger.error(f"STT client connection test failed: {test_err}. Check project ID, API enable status, and credentials.")
            #      STT_CLIENT = None
            #      GOOGLE_CLOUD_AVAILABLE = False
            #      return False

            logger.info("Google Speech-to-Text client initialized.")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize Google STT client: {e}", exc_info=True)
            warnings.warn(f"Could not initialize Google STT Client: {e}. STT features may fail.")
            STT_CLIENT = None
            GOOGLE_CLOUD_AVAILABLE = False # Mark as unavailable on init failure
            return False
    return True # Already initialized

# --- File-Based STT Function ---
def transcribe_audio_file_google(audio_path):
    """
    Transcribes an audio file using Google Cloud Speech-to-Text.

    Args:
        audio_path (str): The path to the audio file.

    Returns:
        tuple: (transcript_text, error_message)
               transcript_text (str): The transcribed text, or message like "[No speech recognized]", or None if a critical error occurred.
               error_message (str): An error message if transcription failed critically, None otherwise.
    """
    global STT_CLIENT
    if not GOOGLE_CLOUD_AVAILABLE or STT_CLIENT is None:
        err = "STT client not available or not initialized."
        logger.error(err)
        return None, err

    if not audio_path or not os.path.exists(audio_path):
        err = f"Audio file not found: '{audio_path}'"
        logger.error(err)
        return None, err

    logger.info(f"Transcribing audio file via Google STT: {os.path.basename(audio_path)}")
    try:
        # Get audio file information (samplerate)
        try:
            info = sf.info(audio_path)
            file_samplerate = info.samplerate
            file_channels = info.channels
            # Attempt to determine encoding from format/subtype - may need more robust check
            file_format = info.format.lower()
            subtype = info.subtype.lower()
            encoding = google_speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED # Default
            if 'wav' in file_format:
                if 'pcm_16' in subtype or 'linear16' in subtype:
                    encoding = google_speech.RecognitionConfig.AudioEncoding.LINEAR16
                elif 'pcm_u8' in subtype or 'ulaw' in subtype:
                    encoding = google_speech.RecognitionConfig.AudioEncoding.MULAW
                # Add more mappings if needed (FLAC, OGG_OPUS, WEBM_OPUS etc.)
            elif 'flac' in file_format:
                 encoding = google_speech.RecognitionConfig.AudioEncoding.FLAC
            elif 'opus' in subtype: # Common for webm/ogg from browser
                 if 'ogg' in file_format:
                      encoding = google_speech.RecognitionConfig.AudioEncoding.OGG_OPUS
                 elif 'webm' in file_format:
                      # Note: WEBM_OPUS might not be directly supported by standard client as of early 2024,
                      # but OGG_OPUS often works or API handles it. Verify with GCP docs.
                      # Using OGG_OPUS as a potential fallback.
                      encoding = google_speech.RecognitionConfig.AudioEncoding.OGG_OPUS
                      logger.info("Detected WEBM with Opus, using OGG_OPUS encoding for STT. Verify compatibility.")
                 else:
                      logger.warning(f"Opus subtype detected in unknown container format: {file_format}. Using unspecified encoding.")
            elif 'amr' in file_format:
                encoding = google_speech.RecognitionConfig.AudioEncoding.AMR if file_samplerate == 8000 else google_speech.RecognitionConfig.AudioEncoding.AMR_WB
            # Add more formats like MP3 if needed (may require specific encoding/sampling rate)

            logger.debug(f"Detected SR: {file_samplerate} Hz, Channels: {file_channels}, Format: {file_format}/{subtype}, Guessed Encoding: {encoding} for {os.path.basename(audio_path)}")

            # Google STT generally prefers mono. Conversion might be needed for best results if stereo.
            if file_channels > 1:
                 logger.warning(f"Audio file has {file_channels} channels. STT prefers mono. Consider conversion if results are poor.")

            # Warn if sample rate is not ideal (though API might handle it)
            if file_samplerate not in [8000, 16000, 44100, 48000] and encoding != google_speech.RecognitionConfig.AudioEncoding.OGG_OPUS: # Opus SR is often determined by content
                logger.warning(f"Audio file sample rate ({file_samplerate}Hz) might not be optimal for the specified encoding ({encoding}). Consider resampling to a standard rate like {config.DEFAULT_AUDIO_SAMPLERATE}Hz.")
                # Override samplerate sent to API? Or let API handle it? Defaulting to detected.
                # file_samplerate = config.DEFAULT_AUDIO_SAMPLERATE

        except Exception as sf_err:
            file_samplerate = config.DEFAULT_AUDIO_SAMPLERATE # Fallback
            encoding = google_speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED
            logger.warning(f"Cannot read audio info from '{os.path.basename(audio_path)}': {sf_err}. Using default SR {file_samplerate}Hz and unspecified encoding.")

        # Read audio content
        with io.open(audio_path, "rb") as audio_file:
            content = audio_file.read()

        if not content:
            err = f"Audio file is empty: '{os.path.basename(audio_path)}'"
            logger.error(err)
            return None, err

        audio = google_speech.RecognitionAudio(content=content)

        # Configure recognition settings
        recognition_config_args = {
            "language_code": config.GOOGLE_STT_LANGUAGE_CODE,
            "sample_rate_hertz": file_samplerate, # Use detected or default SR
            "enable_automatic_punctuation": True,
            # Add model selection if needed (e.g., 'telephony', 'medical_dictation')
            # "model": 'telephony',
            # "audio_channel_count": 1, # Explicitly state mono if converted
        }
        # Only set encoding if we have a specific guess, otherwise let API infer
        if encoding != google_speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED:
            recognition_config_args["encoding"] = encoding

        recognition_config = google_speech.RecognitionConfig(**recognition_config_args)

        logger.info(f"Sending STT request for {os.path.basename(audio_path)} (Size: {len(content)} bytes, Config: {recognition_config_args})...")
        response = STT_CLIENT.recognize(config=recognition_config, audio=audio)
        logger.info(f"Received STT response for {os.path.basename(audio_path)}.")

        # Process response
        if not response.results or not response.results[0].alternatives:
            msg = "[Audio detected - No speech recognized]"
            logger.warning(f"STT could not understand file {os.path.basename(audio_path)}: {msg}")
            return msg, None # Return message indicating no speech, not an error state

        # Get the most likely transcript
        transcript = response.results[0].alternatives[0].transcript.strip()
        confidence = response.results[0].alternatives[0].confidence

        if not transcript:
            msg = "[Audio detected - No speech recognized]"
            logger.warning(f"STT returned empty transcript for {os.path.basename(audio_path)} despite results present: {msg}")
            return msg, None

        logger.info(f"File transcription successful (Conf: {confidence:.2f}): '{transcript[:80]}...'")
        return transcript, None

    except AttributeError as attr_err:
        err = f"Google Speech library components unavailable or STT client issue: {attr_err}"
        logger.error(err, exc_info=True)
        return None, err
    except Exception as e:
        # Catch potential API errors, network issues, etc.
        err = f"Error during STT for {os.path.basename(audio_path)}: {e}"
        logger.error(err, exc_info=True)
        # Check for common API errors (e.g., permissions, quota)
        if "permission denied" in str(e).lower(): err += " (Check API permissions/credentials)"
        elif "quota" in str(e).lower(): err += " (Check API quota)"
        elif "sample_rate_hertz" in str(e).lower() and "invalid" in str(e).lower(): err += f" (Invalid sample rate {file_samplerate}Hz for the detected/specified audio format?)"
        elif "encoding" in str(e).lower() and "invalid" in str(e).lower(): err += f" (Invalid encoding {encoding} for the audio format?)"
        return None, err
# modules/interview_logic.py
import logging
import re
import time
import json
import requests
import socket
import os

# Local module imports
import config
# Use absolute imports within the package if running as a module
from . import utils
from . import llm_interface
from . import prompt_templates
from . import audio_utils # For STT call
from . import report_generator

logger = logging.getLogger(__name__)

# --- Helper to Send Text to NeuroSync Player ---
def send_text_to_player(text_to_send):
    """Sends text to the NeuroSync Player service over TCP."""
    host = config.NEUROSYNC_PLAYER_HOST
    port = config.NEUROSYNC_PLAYER_PORT
    if not text_to_send:
        logger.warning("Attempted to send empty text to player.")
        return False
    logger.info(f"Sending text to NeuroSync Player ({host}:{port}): '{text_to_send[:80]}...'")
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(5.0) # Timeout for connection and send
            s.connect((host, port))
            s.sendall(text_to_send.encode('utf-8'))
            s.shutdown(socket.SHUT_WR) # Signal end of sending

            # Optional: Wait briefly for confirmation (NeuroSync Player needs to send one back)
            try:
                 confirmation = s.recv(1024)
                 if confirmation:
                      logger.info(f"Received confirmation from player: {confirmation.decode('utf-8')}")
                      return True
                 else:
                      # This might happen if player closes connection immediately after receiving
                      logger.warning("No confirmation received from player (but text likely sent).")
                      return True # Assume sent if no error
            except socket.timeout:
                 logger.warning("Timeout waiting for confirmation from player (text likely sent).")
                 return True # Assume sent if no error
            except ConnectionResetError:
                 logger.warning("Player closed connection before confirmation could be read (text likely sent).")
                 return True

    except socket.timeout:
         logger.error(f"Timeout connecting or sending to NeuroSync Player at {host}:{port}.")
         return False
    except ConnectionRefusedError:
        logger.error(f"Connection refused by NeuroSync Player at {host}:{port}. Is it running?")
        return False
    except socket.error as sock_err:
         logger.error(f"Socket error sending text to player: {sock_err}", exc_info=True)
         return False
    except Exception as e:
        logger.error(f"Unexpected error sending text to player: {e}", exc_info=True)
        return False

# --- Helper to Parse Questions ---
def parse_generated_questions(raw_text):
    """
    Parses the LLM response (expected to be a numbered list) to extract questions.
    Removes common LLM preamble/postamble and specific tags used in generation.
    """
    if not raw_text: return []
    questions = []
    # Regex to find lines starting with number., -, *, etc., possibly after tags or whitespace
    # Allow for optional closing punctuation on the number (like ')')
    pattern = re.compile(r"^\s*(?:\[.*?\])?\s*(?:\d{1,2}[\.\)])?\s*(?:[-*•])?\s*(.*)")

    lines = raw_text.strip().split('\n')

    # Expanded list of potential tags/prefixes to remove
    q_type_tags_to_remove = [
        # Specific types from template
        "[Technical/Conceptual]", "[Database Concept]", "[Database Administration]",
        "[Database Concept/Administration]", "[SQL Query]", "[SQL Query Writing]",
        "[SQL Query (Advanced)]", "[Troubleshooting/Problem Solving]", "[Troubleshooting]",
        "[Troubleshooting/Performance]", "[Behavioral/Learning]", "[Coding/Algorithmic]",
        "[System Design]", "[Security Concept]", "[Cloud Concept (if relevant)]",
        "[Behavioral/Teamwork]", "[Behavioral/Problem Solving]", "[DB Concept]",
        "[DB Admin Task]", "[DB Design/Schema]", "[DB Scenario]", "[Performance Scenario]",
        "[Security Scenario]", "[Cloud Scenario]", "[Learning Scenario]", "[Backup/Recovery Scenario]",
        "[Project Deep Dive]", "[Technical Concept/Tradeoff]", "[Coding Challenge (Scenario)]",
        "[System Design (Scenario)]", "[Debugging Scenario]", "[Behavioral Scenario (Teamwork)]",
        "[Behavioral Scenario (Learning)]", "[Technical Scenario]", "[Problem Solving Scenario]",
        "[Tool/Concept Question]", "[Design Question]", "[Behavioral Question]", "[Learning Question]",
        "[DB Concept/Scenario]", "[SQL Query (Scenario)]", "[Troubleshooting Scenario]",
        "[DB Admin Task/Scenario]", "[Security Scenario]", "[Behavioral Scenario (Learning)]",
        # Generic Prefixes often added by LLMs
        "Question:", "Follow-up:", "Next question:", "Okay, next:", "Let's discuss:", "How about:", "Can you explain:",
        "Scenario:", "Task:", "Problem:", "Concept:", "Behavioral:", "Technical:", "Coding:", "Design:"
    ]
    # Common preamble/postamble phrases to ignore completely
    skip_prefixes = ("okay", "great", "thanks", "sure", "understood", "evaluation:", "alignment",
                     "technical accuracy", "relevance", "strengths:", "areas for improvement",
                     "overall score", "here are", "generating", "note:", "based on the", "the question",
                     "interview questions:", "response:", "answer:", "certainly", "here is", "here's")

    for line in lines:
        line_strip = line.strip()
        # Skip empty lines or lines that only contain preamble/postamble
        if not line_strip or any(line_strip.lower().startswith(p) for p in skip_prefixes):
            continue

        match = pattern.match(line_strip)
        if match:
            question_text = match.group(1).strip()
            original_text = question_text # Keep track
            cleaned_text = question_text

            # Iteratively remove known tags/prefixes from the beginning
            cleaned_something = True
            while cleaned_something:
                cleaned_something = False
                for tag in q_type_tags_to_remove:
                    # Case-insensitive match, allowing optional colon/dash and space after tag
                    tag_pattern = r'^\s*' + re.escape(tag) + r'\s*[:\-\s]?\s*'
                    if re.match(tag_pattern, cleaned_text, re.IGNORECASE):
                        new_text = re.sub(tag_pattern, '', cleaned_text, count=1, flags=re.IGNORECASE).strip()
                        if new_text != cleaned_text: # Check if substitution actually happened
                           # logger.debug(f"Removed tag/prefix '{tag}' -> '{new_text}'")
                           cleaned_text = new_text
                           cleaned_something = True
                           break # Restart check with potentially new prefixes revealed

            # Add question if it looks valid
            # Heuristics: not empty, reasonable length, ends with '?' or contains typical question words
            # or if significant cleaning happened.
            if cleaned_text and len(cleaned_text) > 10:
                 q_words = {"what", "how", "why", "explain", "describe", "tell", "compare", "contrast", "give", "scenario"}
                 ends_q = cleaned_text.endswith('?')
                 has_q_word = any(word in cleaned_text.lower() for word in q_words)
                 if ends_q or has_q_word or len(cleaned_text) > 30 or (cleaned_text != original_text):
                      # Prefer cleaned text unless it became suspiciously short/empty
                      final_text = cleaned_text if len(cleaned_text) > 5 else original_text
                      if final_text not in questions: # Avoid duplicates
                           questions.append(final_text)
                 else:
                      # If no strong indicators, but it passed initial regex, maybe keep original?
                      logger.warning(f"Question candidate doesn't strongly look like a question, keeping original: '{original_text}'")
                      if original_text not in questions:
                           questions.append(original_text)

    # Fallback if regex parsing yielded too few results but there are multiple lines
    if not questions and len(lines) > 1:
         logger.warning("Primary regex failed to parse questions, using basic newline/length split fallback.")
         potential_questions = [l.strip() for l in lines if l.strip() and len(l.strip()) > 20 and not any(l.strip().lower().startswith(p) for p in skip_prefixes)]
         # Basic cleaning for fallback
         for q in potential_questions:
             q_cleaned = re.sub(r'^\s*\d+[\.\)]?\s*', '', q).strip() # Remove leading numbers
             if q_cleaned and q_cleaned not in questions:
                 questions.append(q_cleaned)

    questions = [q for q in questions if q] # Final filter for empty strings
    logger.info(f"Parsed {len(questions)} potential questions from LLM generation.")
    return questions


# --- Interview Session Class ---
class InterviewSession:
    def __init__(self, interview_id, resume_text, jd_text):
        self.interview_id = interview_id
        self.resume_text_raw = resume_text
        self.jd_text_raw = jd_text
        self.role_title = "Relevant Role (Check JD)"
        self.resume_summary = ""
        self.jd_summary = ""
        self.project_details = ""
        self.focus_topics = []
        self.prepared_questions = []
        self.conversation_history = [] # List of {"speaker": name, "text": content}
        self.interview_qna = [] # List of turn data dicts for evaluation/report
        self.asked_questions_indices = set() # Tracks indices of prepared questions *successfully asked*
        self.current_turn_number = 0 # Increments when candidate response is processed
        self.state = "INITIALIZING" # INITIALIZING, READY, IN_PROGRESS, ASKING, AWAITING_RESPONSE, EVALUATING, FINISHED, ERROR
        self.last_ai_message = "" # Store the last question/message AI sent
        self.last_question_context = {} # Store info about the question AI just asked for QnA linking
        self.evaluation_complete = False
        self.report_generated = False
        self.report_path = None
        self.error_message = None # Store specific error message if state is ERROR

        self._initialize_session()

    def _set_error_state(self, message):
        """Helper to set the error state and log."""
        logger.error(f"[{self.interview_id}] Error: {message}")
        self.state = "ERROR"
        self.error_message = message
        # Send error message to player? Maybe not, frontend handles displaying errors.
        # send_text_to_player(f"An internal error occurred: {message}")
        self.last_ai_message = f"An error occurred: {message}" # For frontend display

    def _initialize_session(self):
        """Performs initial text analysis and question generation."""
        logger.info(f"[{self.interview_id}] Initializing interview session...")
        try:
            # 1. Summarize and Extract Details (Basic cleaning)
            self.resume_summary = utils.clean_text(self.resume_text_raw[:config.MAX_SUMMARY_LENGTH * 2])[:config.MAX_SUMMARY_LENGTH]
            self.jd_summary = utils.clean_text(self.jd_text_raw[:config.MAX_SUMMARY_LENGTH * 2])[:config.MAX_SUMMARY_LENGTH]
            # Use the raw text for project details extraction as cleaning might remove structure
            self.project_details = utils.extract_project_details(self.resume_text_raw)
            if len(self.project_details) > config.MAX_PROJECT_SUMMARY_LENGTH:
                 self.project_details = self.project_details[:config.MAX_PROJECT_SUMMARY_LENGTH] + "... (truncated)"
            logger.info(f"[{self.interview_id}] Input texts summarized. Extracted {len(self.project_details)} chars of project/experience details.")

            # 2. Identify Role and Focus Topics
            role_match = re.search(r"^(?:Job\s+)?Title\s*[:\-]?\s*(.*?)(\n|$)", self.jd_text_raw, re.IGNORECASE | re.MULTILINE)
            self.role_title = role_match.group(1).strip() if role_match else "Relevant Role (from Job Description)"
            logger.info(f"[{self.interview_id}] Identified Role Title: {self.role_title}")
            # Ensure focus topics are relevant and not too generic
            self.focus_topics = utils.get_focus_topics(self.resume_text_raw, self.jd_text_raw, top_n=5)
            logger.info(f"[{self.interview_id}] Identified Focus Topics: {self.focus_topics}")

            # 3. Prepare RAG Context (Optional)
            rag_context = "No RAG context available or enabled."
            # RAG logic remains the same as before...
            if config.RETRIEVAL_TOP_K > 0:
                # Ensure embedding model is loaded (utils.py should handle this)
                if not utils.embedding_model:
                    logger.warning(f"[{self.interview_id}] RAG enabled but embedding model not loaded. Skipping retrieval.")
                else:
                    search_queries = utils.generate_search_queries(self.resume_summary, self.jd_summary) # Use summaries
                    if search_queries:
                        all_retrieved_docs = []
                        logger.info(f"[{self.interview_id}] Retrieving RAG context for {len(search_queries)} queries...")
                        for query in search_queries:
                            docs = utils.retrieve_similar_documents(query, top_k=config.RETRIEVAL_TOP_K, threshold=config.RETRIEVAL_SIMILARITY_THRESHOLD)
                            if docs: all_retrieved_docs.extend(docs)

                        if all_retrieved_docs:
                            unique_docs_dict = {doc['content']: doc for doc in all_retrieved_docs} # Simple dedupe by content
                            sorted_unique_docs = sorted(unique_docs_dict.values(), key=lambda x: x.get("score", 0), reverse=True)
                            rag_context = utils.format_rag_context(sorted_unique_docs, max_length=config.MAX_CONTEXT_LENGTH)
                            logger.info(f"[{self.interview_id}] RAG context prepared (length: {len(rag_context)} chars).")
                        else:
                            logger.warning(f"[{self.interview_id}] No relevant documents found in knowledge base for RAG.")
                    else:
                        logger.warning(f"[{self.interview_id}] Could not generate search queries for RAG.")
            else:
                logger.info(f"[{self.interview_id}] Skipping RAG context retrieval (RETRIEVAL_TOP_K <= 0).")


            # 4. Determine Role-Specific Guidance (Refined based on role title check)
            role_lower = self.role_title.lower()
            if "database admin" in role_lower or "dba" in role_lower or "database administrator" in role_lower:
                q_types_list = ["[DB Concept/Scenario]", "[SQL Query (Scenario)]", "[Troubleshooting Scenario]", "[DB Admin Task/Scenario]", "[Security Scenario]", "[Behavioral/Learning Scenario]"]
                role_guidance = {"role": "Probe core DB concepts, backup/recovery, performance tuning.", "code": "Focus on practical SQL for administration & querying.", "solve": "Present common DBA challenges (e.g., locking, slow queries, disk space)."}
            elif "software engineer" in role_lower or "developer" in role_lower or "programmer" in role_lower:
                q_types_list = ["[Technical Concept/Tradeoff]", "[Coding Challenge (Scenario)]", "[System Design (Scenario)]", "[Debugging Scenario]", "[Behavioral Scenario (Teamwork)]", "[Behavioral Scenario (Learning)]"]
                role_guidance = {"role": "Assess CS fundamentals, data structures, algorithms.", "code": "Provide small coding problems (logic, syntax).", "solve": "Debugging/design scenarios related to application development."}
            else: # Default / Analyst / Other
                q_types_list = ["[Technical Scenario]", "[Problem Solving Scenario]", "[Tool/Concept Question]", "[Data Interpretation (if relevant)]", "[Behavioral Question]", "[Learning Question]"]
                role_guidance = {"role": "Focus on general tech concepts relevant to the JD.", "code": "Ask about high-level logic or specific tool usage.", "solve": "Present general technical or analytical challenges."}
            logger.info(f"[{self.interview_id}] Using role guidance for '{role_lower}': {role_guidance}")


            # 5. Prepare Prompt Arguments for Question Generation
            num_questions_total = config.NUM_QUESTIONS + 1 # +1 for project deep dive
            qg_prompt_args = {
                "role_title": self.role_title,
                "resume_summary": self.resume_summary or "Not provided.",
                "jd_summary": self.jd_summary or "Not provided.",
                "project_details": self.project_details or "No specific project details extracted.",
                "focus_str": ', '.join(self.focus_topics) if self.focus_topics else f'General skills for {self.role_title}',
                "context_str": rag_context,
                "num_questions": config.NUM_QUESTIONS, # Number of main questions
                "num_questions_plus_one": num_questions_total, # Total including deep dive
                "role_specific_guidance": role_guidance["role"],
                "coding_guidance": role_guidance["code"],
                "problem_solving_guidance": role_guidance["solve"],
                "extra_hints_str": "Ensure questions are distinct and progressively probe deeper if appropriate.",
            }

            # Dynamically generate the structure for the question list in the prompt
            q_lines_for_template = ""
            for i in range(num_questions_total):
                q_index = i % len(q_types_list) # Cycle through base types
                # Assign a specific type, ensure last one is Project Deep Dive
                q_type = "[Project Deep Dive]" if i == config.NUM_QUESTIONS else q_types_list[q_index]
                qg_prompt_args[f"q_type_{i}"] = q_type
                q_lines_for_template += f"{i+1}. {q_type} ...\n" # Let LLM fill in the question text

            # Find the placeholder section in the template and replace it
            # Making the pattern more robust to find the start and end of the example list
            placeholder_pattern = re.compile(
                 r"(?:Here is the desired format:?\s*\n)?\s*1\.\s*\[.*?\]\s*\.\.\..*?\n\s*" + # Start of example list
                 str(num_questions_total) + r"\.\s*\[.*?\]\s*\.\.\.", # End of example list
                 re.DOTALL | re.IGNORECASE
            )

            if placeholder_pattern.search(prompt_templates.QUESTION_GENERATION_PROMPT_TEMPLATE):
                 modified_qg_template = placeholder_pattern.sub(
                      f"Here is the desired format:\n{q_lines_for_template.strip()}", # Replace with dynamic lines
                      prompt_templates.QUESTION_GENERATION_PROMPT_TEMPLATE,
                      count=1 # Replace only the first occurrence
                 )
                 logger.debug(f"[{self.interview_id}] Modified question generation template with dynamic types.")
            else:
                 logger.warning(f"[{self.interview_id}] Could not find placeholder pattern in question generation template. Using original template.")
                 modified_qg_template = prompt_templates.QUESTION_GENERATION_PROMPT_TEMPLATE
                 # Add the args anyway, they might be used elsewhere in the template
                 modified_qg_template += "\n\n# Question Types:\n" + q_lines_for_template

            # Format the final prompt
            try:
                 question_gen_prompt = modified_qg_template.format(**qg_prompt_args)
            except KeyError as fmt_err:
                 self._set_error_state(f"Missing key in question generation prompt template: {fmt_err}")
                 return

            # 6. Call LLM to Generate Questions
            logger.info(f"[{self.interview_id}] Generating ~{num_questions_total} interview questions via LLM ({config.INTERVIEWER_LLM_MODEL_NAME})...")
            # Use a higher token limit for generation as it includes context + questions
            generation_max_tokens = max(config.INTERVIEWER_MAX_TOKENS * 2, 1500)
            raw_questions_text = llm_interface.query_llm(
                question_gen_prompt, config.INTERVIEWER_LLM_MODEL_NAME,
                generation_max_tokens,
                config.INTERVIEWER_TEMPERATURE
            )

            if raw_questions_text is None or raw_questions_text.startswith("Error:"):
                error_detail = raw_questions_text if raw_questions_text else "LLM call failed."
                self._set_error_state(f"Failed to generate initial questions. Details: {error_detail}")
                return

            cleaned_questions_text = llm_interface.clean_llm_output(raw_questions_text)
            self.prepared_questions = parse_generated_questions(cleaned_questions_text)

            # Validate number of questions generated
            if not self.prepared_questions or len(self.prepared_questions) < config.NUM_QUESTIONS: # Check for at least the core number
                logger.error(f"[{self.interview_id}] Failed to parse sufficient questions (expected ~{num_questions_total}, found {len(self.prepared_questions)}). Raw: '{raw_questions_text[:150]}...' Cleaned: '{cleaned_questions_text[:150]}...'")
                # Attempt to use whatever was parsed if > 0? Or fail? Let's fail for consistency.
                self._set_error_state(f"Could not parse the generated questions (expected ~{num_questions_total}, found {len(self.prepared_questions)}). Check LLM output/parsing logic.")
                return

            # Trim excess if LLM generated too many
            self.prepared_questions = self.prepared_questions[:num_questions_total]
            logger.info(f"[{self.interview_id}] Successfully generated and parsed {len(self.prepared_questions)} questions.")
            logger.debug(f"[{self.interview_id}] Prepared Questions: {self.prepared_questions}")

            self.state = "READY"
            logger.info(f"[{self.interview_id}] Interview session initialized and ready.")

        except Exception as e:
            logger.exception(f"[{self.interview_id}] Unexpected error during session initialization: {e}")
            self._set_error_state(f"An unexpected error occurred during interview setup: {e}")

    def get_greeting(self):
        """Returns the initial greeting message and transitions state."""
        if self.state != "READY":
            logger.warning(f"[{self.interview_id}] Attempted to get greeting but state is {self.state}")
            return self.last_ai_message or "Interview is not ready."

        greeting_text = f"Hello {config.CANDIDATE_NAME}. I'm {config.INTERVIEWER_AI_NAME}, and I'll be conducting your interview today for the {self.role_title} position at {config.COMPANY_NAME}. We'll go through about {len(self.prepared_questions)} questions covering technical concepts and your past experience. Please respond verbally when prompted. Are you ready to begin?"

        logger.info(f"[{self.interview_id}] Sending greeting to player and frontend.")
        if not send_text_to_player(greeting_text):
             # If sending fails, maybe don't proceed? Or log warning and proceed?
             logger.warning(f"[{self.interview_id}] Failed to send greeting to NeuroSync Player. Continuing frontend flow.")

        self.last_ai_message = greeting_text
        self.conversation_history.append({"speaker": config.INTERVIEWER_AI_NAME, "text": greeting_text})
        self.state = "AWAITING_RESPONSE" # Wait for user confirmation / first response
        self.current_turn_number = 0 # Turn 0 is greeting, turn 1 starts with first real question
        # No question context set yet for the greeting
        self.last_question_context = {
             "question": "Initial Greeting / Ready Check",
             "is_prepared": False,
             "prepared_index": None,
             "detection_method": "Greeting",
             "turn": 0
         }
        return greeting_text

    def get_next_ai_turn(self):
        """
        Generates the AI interviewer's next response or question.
        This should be called *after* processing the candidate's response (or initially after greeting confirmation).
        """
        if self.state not in ["ASKING", "IN_PROGRESS"]: # Should be triggered internally or after response
            # Allow calling if state is AWAITING_RESPONSE and history suggests user confirmed ready
             if self.state == "AWAITING_RESPONSE" and len(self.conversation_history) > 1 and self.conversation_history[-1]['speaker'] == config.CANDIDATE_NAME:
                 logger.info(f"[{self.interview_id}] Proceeding to first question after user confirmation.")
                 self.state = "ASKING" # Set state to indicate AI is about to ask
             else:
                 logger.warning(f"[{self.interview_id}] Attempted to get next AI turn but state is {self.state}. Expected ASKING or IN_PROGRESS.")
                 return self.last_ai_message or "Interview state invalid for generating next AI turn."

        turn = self.current_turn_number + 1 # The turn number AI is about to start
        logger.info(f"[{self.interview_id}] --- Starting AI Turn {turn} ---")
        self.state = "ASKING" # Ensure state reflects AI is thinking/asking

        # Check if we should conclude the interview
        # If all prepared questions asked AND it's beyond the expected number of turns
        asked_count = len(self.asked_questions_indices)
        if asked_count >= len(self.prepared_questions) and turn > len(self.prepared_questions):
             # Make sure the candidate actually responded to the last question
             if not self.conversation_history or self.conversation_history[-1]['speaker'] == config.CANDIDATE_NAME:
                  logger.info(f"[{self.interview_id}] All prepared questions asked ({asked_count}/{len(self.prepared_questions)}). Preparing closing remarks.")
                  closing_text = f"Alright, that concludes our planned questions. Thank you very much for your time and for sharing your experience, {config.CANDIDATE_NAME}. We'll evaluate the session and be in touch regarding the next steps."
                  send_text_to_player(closing_text)
                  self.last_ai_message = closing_text
                  self.conversation_history.append({"speaker": config.INTERVIEWER_AI_NAME, "text": closing_text})
                  self.state = "FINISHED" # Move to finished state, evaluation will follow
                  return closing_text
             else:
                  # AI spoke last, possibly a follow-up after the last question. Let's allow one more cycle or end here.
                  logger.info(f"[{self.interview_id}] All prepared questions asked, AI spoke last. Ending interview.")
                  # Use previous closing message if it exists, otherwise generate a short one.
                  if self.last_ai_message and ("concludes" in self.last_ai_message or "thank you" in self.last_ai_message.lower()):
                      # Re-send or just return existing message? Let's return it.
                      # send_text_to_player(self.last_ai_message) # Resend if needed
                      self.state = "FINISHED"
                      return self.last_ai_message
                  else:
                      closing_text = f"Thank you again, {config.CANDIDATE_NAME}. That's all the questions I have for now. We will be in touch."
                      send_text_to_player(closing_text)
                      self.last_ai_message = closing_text
                      self.conversation_history.append({"speaker": config.INTERVIEWER_AI_NAME, "text": closing_text})
                      self.state = "FINISHED"
                      return closing_text

        # --- Prepare Prompt for Conversational Turn ---
        # Format conversation history for the prompt
        history_str = self._format_conversation_history(max_turns=6) # Limit context window
        # Identify remaining prepared questions
        remaining_indices = [i for i in range(len(self.prepared_questions)) if i not in self.asked_questions_indices]
        asked_str = ", ".join(str(i+1) for i in sorted(list(self.asked_questions_indices))) or "None yet"
        remaining_str = ", ".join(str(i+1) for i in remaining_indices) or "None (proceed with follow-ups or conclude)"

        # Prepare prompt arguments
        conv_prompt_args = {
            "interviewer_name": config.INTERVIEWER_AI_NAME,
            "company_name": config.COMPANY_NAME,
            "role_title": self.role_title,
            "candidate_name": config.CANDIDATE_NAME,
            "resume_summary": self.resume_summary,
            "jd_summary": self.jd_summary,
            "project_details": self.project_details,
            "focus_topics_str": ', '.join(self.focus_topics),
            "prepared_questions_numbered": "\n".join(f"{i+1}. {q}" for i, q in enumerate(self.prepared_questions)),
            "asked_questions_str": asked_str,
            "remaining_questions_str": remaining_str,
            "conversation_history": history_str,
            "current_turn_number": turn,
            "total_questions_planned": len(self.prepared_questions)
        }
        try:
            interview_turn_prompt = prompt_templates.CONVERSATIONAL_INTERVIEW_PROMPT_TEMPLATE.format(**conv_prompt_args)
        except KeyError as fmt_err:
            self._set_error_state(f"Missing key in conversational prompt template: {fmt_err}")
            return self.last_ai_message # Return previous message or error

        # Call LLM for AI response
        logger.debug(f"[{self.interview_id}] Sending prompt to interviewer LLM (Turn {turn}). History length: {len(history_str)} chars.")
        ai_response_raw = llm_interface.query_llm(
            interview_turn_prompt, config.INTERVIEWER_LLM_MODEL_NAME,
            config.INTERVIEWER_MAX_TOKENS, config.INTERVIEWER_TEMPERATURE
        )
        ai_response = llm_interface.clean_llm_output(ai_response_raw)

        if ai_response is None or ai_response.startswith("Error:"):
            error_detail = ai_response if ai_response else "LLM call failed."
            # Don't set error state here, just return an apology message? Or should it halt?
            # Let's try to recover by returning an apology and potentially allowing retry?
            logger.error(f"[{self.interview_id}] Interviewer LLM failed on turn {turn}: {error_detail}")
            error_text = f"Apologies, I encountered a temporary issue generating my response. Could you perhaps repeat your last point or should we try the next question?"
            # Don't change main state, keep it as ASKING? Frontend might retry getAiMessage.
            # self.state = "ERROR" # Avoid halting unless it's persistent
            self.last_ai_message = error_text # Update message for frontend display
            send_text_to_player(error_text) # Send apology to player
            return error_text # Return apology to frontend

        # --- Detect if AI asked a Prepared Question ---
        identified_prepared_index = -1
        current_question_text_for_eval = ai_response # Default: use full AI response as context
        detected_method = "Follow-up or Transition"

        if remaining_indices:
            # Use a more robust matching (e.g., simple keyword overlap + structure)
            best_match_index, highest_overlap = self._find_best_question_match(ai_response, remaining_indices)

            if best_match_index != -1:
                 identified_prepared_index = best_match_index
                 # Use the canonical prepared question text for evaluation context
                 current_question_text_for_eval = self.prepared_questions[identified_prepared_index]
                 detected_method = f"Prepared Q Match (Overlap: {highest_overlap:.1%})"
                 logger.info(f"[{self.interview_id}] Detected prepared question {identified_prepared_index + 1}: '{current_question_text_for_eval[:50]}...'. Method: {detected_method}")
                 # Mark as asked *only when detected*
                 self.asked_questions_indices.add(identified_prepared_index)
            else:
                 logger.info(f"[{self.interview_id}] AI response didn't strongly match remaining prepared questions ({remaining_indices}). Assuming follow-up/transition.")
                 # If no match, the current_question_text_for_eval remains the full AI response
                 # Consider if we should try to extract *just* the question part from ai_response?
                 # Simple approach: if ends with '?', take the last sentence.
                 sentences = re.split(r'(?<=[.!?])\s+', ai_response.strip())
                 if sentences and sentences[-1].endswith('?') and len(sentences[-1]) > 15:
                      current_question_text_for_eval = sentences[-1]
                      logger.debug(f"[{self.interview_id}] Using last sentence as question context: '{current_question_text_for_eval[:60]}...'")

        else:
             logger.info(f"[{self.interview_id}] No prepared questions remaining. AI response is likely a follow-up or closing.")
             # Again, try to extract the question part if relevant
             sentences = re.split(r'(?<=[.!?])\s+', ai_response.strip())
             if sentences and sentences[-1].endswith('?') and len(sentences[-1]) > 15:
                  current_question_text_for_eval = sentences[-1]
                  logger.debug(f"[{self.interview_id}] Using last sentence as follow-up question context: '{current_question_text_for_eval[:60]}...'")

        # Store AI response in history
        self.conversation_history.append({"speaker": config.INTERVIEWER_AI_NAME, "text": ai_response})
        self.last_ai_message = ai_response # Store the actual AI message sent

        # Store context about the question that was just asked (for linking the candidate's *next* response)
        self._set_last_question_for_eval(current_question_text_for_eval, identified_prepared_index, detected_method, turn)

        # Send response to NeuroSync Player
        if not send_text_to_player(ai_response):
             logger.warning(f"[{self.interview_id}] Failed to send AI turn {turn} message to NeuroSync Player.")

        # Transition state to wait for the candidate's response
        self.state = "AWAITING_RESPONSE"
        logger.info(f"[{self.interview_id}] AI Turn {turn} complete. State -> AWAITING_RESPONSE.")
        return ai_response # Return the text to be displayed on the frontend

    def _find_best_question_match(self, ai_response, remaining_indices):
        """Helper to find the best matching prepared question in the AI response."""
        ai_response_lower = ai_response.lower()
        best_match_index = -1
        highest_overlap = 0.0
        # Use simple word overlap of non-stopwords (can be improved)
        try:
             from nltk.corpus import stopwords
             stop_words = set(stopwords.words('english'))
        except LookupError:
             logger.warning("NLTK stopwords not found. Download them ('nltk.download(\"stopwords\")'). Proceeding without stopword removal for matching.")
             stop_words = set()
        except ImportError:
             logger.warning("NLTK not installed. Proceeding without stopword removal for matching.")
             stop_words = set()

        # Consider only longer words after removing punctuation
        ai_words = set(w for w in re.findall(r'\b\w{3,}\b', ai_response_lower) if w not in stop_words)
        if not ai_words: return -1, 0.0 # Cannot match if AI response has no usable words

        for i in remaining_indices:
            q_text = self.prepared_questions[i]
            q_text_lower = q_text.lower()
            q_words = set(w for w in re.findall(r'\b\w{3,}\b', q_text_lower) if w not in stop_words)
            if not q_words: continue

            common_words = q_words.intersection(ai_words)
            # Calculate Jaccard index or simple overlap relative to question length
            # overlap_ratio = len(common_words) / len(q_words.union(ai_words)) # Jaccard
            overlap_ratio = len(common_words) / len(q_words) if len(q_words) > 0 else 0 # Simple overlap

            # Threshold and find best match
            # Requires a reasonably high overlap (e.g., > 50-60%) AND maybe keyword check?
            # Or check if AI response *contains* a large chunk of the question?
            # Let's stick to overlap ratio for now. Adjust threshold (0.55 - 0.65 might be reasonable)
            match_threshold = 0.60
            if overlap_ratio > highest_overlap and overlap_ratio >= match_threshold:
                highest_overlap = overlap_ratio
                best_match_index = i

        return best_match_index, highest_overlap

    def _set_last_question_for_eval(self, question_text, index, method, turn_asked):
         """Stores the context for the candidate's upcoming answer."""
         self.last_question_context = {
              "question": question_text,
              "is_prepared": index != -1,
              "prepared_index": index + 1 if index != -1 else None, # Use 1-based index for reporting
              "detection_method": method,
              "turn": turn_asked # The turn number when the AI *asked* this question
         }
         logger.debug(f"[{self.interview_id}] Set question context for upcoming response (AI Turn {turn_asked}): Prepared={self.last_question_context['is_prepared']}, Index={self.last_question_context['prepared_index']}, Text='{question_text[:60]}...'")


    def _format_conversation_history(self, max_turns=8):
        """Formats the recent history list into a string for the LLM prompt."""
        if not self.conversation_history: return "The conversation has not started yet."
        formatted = ""
        # Get the last N turns (each turn ideally has AI + Candidate = 2 entries, but handle variation)
        # Limit by number of entries to avoid overly long history context
        max_entries = max_turns * 2
        recent_history = self.conversation_history[-max_entries:]
        for entry in recent_history:
            speaker = entry.get('speaker', 'Unknown')
            text = entry.get('text', '[No text provided]')
            # Truncate long responses in history to keep prompt focused
            text_snippet = (text[:200] + '...') if len(text) > 200 else text
            formatted += f"**{speaker}:** {text_snippet}\n\n"
        return formatted.strip()

    def process_candidate_response(self, audio_file_path):
        """Processes the uploaded candidate audio response."""
        if self.state != "AWAITING_RESPONSE":
            # Allow processing if state is ERROR? Maybe not.
            logger.warning(f"[{self.interview_id}] Received candidate response but state is {self.state}. Expected AWAITING_RESPONSE.")
            return {"status": "error", "message": f"Cannot process response now, current state is {self.state}."}

        # This response corresponds to the question asked in the *previous* AI turn.
        # The turn number associated with this Q&A pair is the turn number when the question was asked.
        qna_turn_number = self.last_question_context.get('turn', self.current_turn_number) # Use context if available

        logger.info(f"[{self.interview_id}] Processing candidate response for Q asked in turn {qna_turn_number} from: {os.path.basename(audio_file_path)}")
        self.state = "IN_PROGRESS" # Indicate processing is happening

        # 1. Transcribe Audio using STT
        candidate_response_text, stt_error = audio_utils.transcribe_audio_file_google(audio_file_path)

        # Handle STT Outcomes
        if stt_error:
            logger.error(f"[{self.interview_id}] STT failed for turn {qna_turn_number}: {stt_error}")
            # Decide how to represent this - use error message or a placeholder?
            candidate_response_text = f"[STT Error: {stt_error}]"
            stt_success = False
        elif candidate_response_text is None:
             # This case should ideally not happen if transcribe_audio_file_google returns error message or "[No speech...]"
             logger.error(f"[{self.interview_id}] STT returned None unexpectedly for turn {qna_turn_number}")
             candidate_response_text = "[STT Error: Transcription failed silently]"
             stt_success = False
             stt_error = "Transcription returned None" # Synthesize error message
        elif candidate_response_text == "[Audio detected - No speech recognized]":
             logger.warning(f"[{self.interview_id}] No speech detected by STT for turn {qna_turn_number}.")
             stt_success = True # STT worked, just no speech
        else:
            logger.info(f"[{self.interview_id}] Turn {qna_turn_number} Transcription: '{candidate_response_text[:100]}...'")
            stt_success = True # Transcription successful

        # Add transcription to conversation history immediately
        self.conversation_history.append({"speaker": config.CANDIDATE_NAME, "text": candidate_response_text})

        # 2. Call Emotion Analysis API (if STT was successful and yielded speech)
        confidence_results = {'score': None, 'rating': "N/A", 'primary_emotion': "N/A", 'error': True, 'message': 'Analysis not performed'}
        if stt_success and candidate_response_text != "[Audio detected - No speech recognized]":
            logger.info(f"[{self.interview_id}] Calling Emotion Analysis API ({config.EMOTION_API_ENDPOINT}) for turn {qna_turn_number} audio: {os.path.basename(audio_file_path)}")
            abs_audio_path = os.path.abspath(audio_file_path) # Ensure absolute path
            api_payload = {'audio_path': abs_audio_path}
            try:
                # Increased timeout for potentially longer analysis
                api_response = requests.post(config.EMOTION_API_ENDPOINT, json=api_payload, timeout=60)
                api_response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
                response_data = api_response.json()
                # Check for application-level error within the JSON response
                if response_data.get('error'):
                    confidence_results['message'] = response_data.get('message', 'Emotion API reported an analysis error')
                    logger.warning(f"[{self.interview_id}] Emotion API Error (Turn {qna_turn_number}): {confidence_results['message']}")
                else:
                    confidence_results = response_data # Use the successful result
                    confidence_results['error'] = False # Explicitly mark as success
                    logger.info(f"[{self.interview_id}] Emotion API Analysis successful for turn {qna_turn_number}. Score: {confidence_results.get('score')}")
                # Ensure essential keys exist even on error reported by API
                confidence_results.setdefault('score', None)
                confidence_results.setdefault('rating', 'N/A')
                confidence_results.setdefault('primary_emotion', 'N/A')

            except requests.exceptions.ConnectionError as conn_err:
                confidence_results['message'] = f'Emotion API Connection Error: {conn_err}'
                logger.error(f"[{self.interview_id}] {confidence_results['message']}")
            except requests.exceptions.Timeout:
                confidence_results['message'] = 'Emotion API Timeout'
                logger.error(f"[{self.interview_id}] {confidence_results['message']}")
            except requests.exceptions.RequestException as req_err:
                 status_code = getattr(req_err.response, 'status_code', 'N/A')
                 response_text = getattr(req_err.response, 'text', 'N/A')[:200]
                 confidence_results['message'] = f'Emotion API Request Error: Status {status_code}. Response: {response_text}'
                 logger.error(f"[{self.interview_id}] {confidence_results['message']}", exc_info=True)
            except json.JSONDecodeError:
                 confidence_results['message'] = 'Emotion API Invalid JSON Response'
                 logger.error(f"[{self.interview_id}] {confidence_results['message']}")
            except Exception as api_call_err:
                 confidence_results['message'] = f'Unexpected error calling Emotion API: {api_call_err}'
                 logger.error(f"[{self.interview_id}] {confidence_results['message']}", exc_info=True)
        elif not stt_success:
             confidence_results['message'] = f'Skipped due to STT Error: {stt_error}'
             logger.info(f"[{self.interview_id}] Skipping emotion analysis for turn {qna_turn_number} due to STT error.")
        else: # No speech detected case
             confidence_results['message'] = 'Skipped (No speech detected)'
             confidence_results['error'] = False # Not an API error, just skipped
             logger.info(f"[{self.interview_id}] Skipping emotion analysis for turn {qna_turn_number} (No speech detected).")


        # 3. Store Turn Data for Evaluation and Report
        # Retrieve the question context saved when the AI asked the question
        last_q_context = self.last_question_context
        if not last_q_context or last_q_context.get('turn') != qna_turn_number:
             logger.warning(f"[{self.interview_id}] Mismatch between QnA turn ({qna_turn_number}) and last question context turn ({last_q_context.get('turn')}). Using fallback question context.")
             # Fallback: try to find the last message from the AI in history
             ai_messages = [h['text'] for h in self.conversation_history if h['speaker'] == config.INTERVIEWER_AI_NAME]
             question_for_qna = ai_messages[-1] if ai_messages else "Unknown Question (Context Mismatch)"
             is_prepared = False
             prep_q_idx = None
             detect_method = "Context Mismatch Fallback"
        else:
             question_for_qna = last_q_context['question']
             is_prepared = last_q_context['is_prepared']
             prep_q_idx = last_q_context['prepared_index']
             detect_method = last_q_context['detection_method']

        qna_data = {
            "question_turn": qna_turn_number,
            "question": question_for_qna,
            "response": candidate_response_text, # The transcribed text or error message
            "is_prepared_question": is_prepared,
            "prepared_question_index": prep_q_idx,
            "detection_method": detect_method,
            "stt_success": stt_success, # Boolean indicating if STT worked
            "stt_error_message": stt_error, # Specific error message if stt_success is False
            # --- Confidence Results ---
            "confidence_score": confidence_results.get('score'),
            "confidence_rating": confidence_results.get('rating', 'N/A'),
            "primary_emotion": confidence_results.get('primary_emotion', 'N/A'),
            "confidence_analysis_error": confidence_results.get('error', True), # True if API call failed OR API reported error
            "confidence_message": confidence_results.get('message', 'Analysis not performed'),
            # --- Evaluation Results (Filled later) ---
            "evaluation": None,
            "score": None,
            "score_justification": None,
        }
        self.interview_qna.append(qna_data)

        # Increment the main turn number counter AFTER processing the response
        self.current_turn_number = qna_turn_number # Align counter with the completed turn

        # Set state ready for the AI's next turn
        self.state = "ASKING" # Ready for get_next_ai_turn() to be called
        logger.info(f"[{self.interview_id}] Finished processing candidate response for QnA turn {qna_turn_number}. State -> ASKING.")
        return {"status": "success", "message": "Response processed."}

    def perform_final_evaluation(self):
        """Evaluates all recorded text responses using the evaluator LLM."""
        if self.state not in ["FINISHED", "EVALUATING"]: # Can only evaluate when interview flow is done
             logger.warning(f"[{self.interview_id}] Cannot evaluate, interview state is {self.state}. Must be FINISHED.")
             return False
        if self.evaluation_complete:
             logger.info(f"[{self.interview_id}] Evaluation already performed.")
             return True

        logger.info(f"[{self.interview_id}] Starting final evaluation of {len(self.interview_qna)} recorded QnA pairs...")
        self.state = "EVALUATING"
        evaluation_errors = 0

        for i, item in enumerate(self.interview_qna):
            q_text = item["question"]
            c_response_text = item["response"]
            turn = item["question_turn"]

            # Skip evaluation if STT failed or no meaningful response was captured
            if not item["stt_success"] or c_response_text == "[Audio detected - No speech recognized]":
                 skip_reason = item['stt_error_message'] or 'No speech detected'
                 logger.warning(f"[{self.interview_id}] Skipping text evaluation for turn {turn} due to: {skip_reason}")
                 item["evaluation"] = f"Evaluation skipped ({skip_reason})"
                 item["score"] = None
                 item["score_justification"] = "N/A"
                 continue # Move to the next item

            logger.info(f"[{self.interview_id}] Evaluating response text for turn {turn} ({i+1}/{len(self.interview_qna)})...")
            eval_prompt_args = {
                 "role_title": self.role_title,
                 "jd_summary": self.jd_summary,
                 "resume_summary": self.resume_summary,
                 "interview_question": q_text,
                 "candidate_response": c_response_text
            }
            try:
                evaluation_prompt = prompt_templates.EVALUATION_PROMPT_TEMPLATE.format(**eval_prompt_args)
            except KeyError as fmt_err:
                 logger.error(f"[{self.interview_id}] Skipping evaluation for turn {turn}: Missing key in evaluation prompt template: {fmt_err}")
                 item["evaluation"] = f"Evaluation Error: Prompt template key error ({fmt_err})"
                 item["score"] = None
                 item["score_justification"] = "N/A"
                 evaluation_errors += 1
                 continue

            evaluation_raw = llm_interface.query_llm(
                 evaluation_prompt, config.EVALUATOR_LLM_MODEL_NAME,
                 config.EVALUATOR_MAX_TOKENS, config.EVALUATOR_TEMPERATURE
            )
            evaluation = llm_interface.clean_llm_output(evaluation_raw, is_evaluation=True)

            if evaluation is None or evaluation.startswith("Error:"):
                 error_detail = evaluation if evaluation else "LLM call failed."
                 logger.error(f"[{self.interview_id}] Evaluator LLM failed for turn {turn}: {error_detail}")
                 item["evaluation"] = f"Evaluation Error: {error_detail}"
                 item["score"] = None
                 item["score_justification"] = "N/A"
                 evaluation_errors += 1
            else:
                 item["evaluation"] = evaluation
                 # Parse score and justification from the evaluation text
                 # Making regex more robust to variations (e.g., "Score: 4/5", "Score (1-5): 3")
                 score_match = re.search(r"Overall Score\s*(?:\(1-5\)|out of 5)?\s*[:\-]?\s*([1-5])(?:/\s*5)?", evaluation, re.IGNORECASE)
                 just_match = re.search(r"Justification\s*[:\-]?\s*(.*)", evaluation, re.IGNORECASE | re.DOTALL)

                 if score_match:
                      item["score"] = int(score_match.group(1))
                      logger.info(f"[{self.interview_id}] Parsed score for turn {turn}: {item['score']}")
                 else:
                      item["score"] = None
                      logger.warning(f"[{self.interview_id}] Could not parse score (1-5) for turn {turn}. Evaluation text: '{evaluation[:100]}...'")

                 if just_match:
                      # Clean up justification text: take everything after "Justification:" until the next potential section or end of string
                      just_text = just_match.group(1).strip()
                      # Stop justification if another common evaluation section header starts on a new line
                      # Be careful not to cut off multi-paragraph justifications
                      stop_patterns = [
                           r"\n\s*(?:Strengths|Areas for Improvement|Suggestions|Alignment|Technical Accuracy|Relevance|Overall Assessment)\s*:",
                           r"\n\s*[-*•\d]+\s+" # Stop if a new list item starts
                      ]
                      for pattern in stop_patterns:
                           match = re.search(pattern, just_text, re.IGNORECASE)
                           if match:
                                just_text = just_text[:match.start()].strip()
                      item["score_justification"] = just_text if just_text else "N/A"
                      logger.debug(f"[{self.interview_id}] Parsed justification for turn {turn}: {item['score_justification'][:60]}...")
                 else:
                      item["score_justification"] = "N/A"
                      logger.warning(f"[{self.interview_id}] Could not parse justification for turn {turn}. Evaluation text: '{evaluation[:100]}...'")

            # Optional delay to avoid hitting API rate limits
            time.sleep(0.5) # Adjust as needed

        self.evaluation_complete = True
        # Keep state as EVALUATING or FINISHED? Let's keep it FINISHED as evaluation is post-interview.
        self.state = "FINISHED"
        if evaluation_errors > 0:
             logger.warning(f"[{self.interview_id}] Evaluation phase completed with {evaluation_errors} errors.")
        else:
             logger.info(f"[{self.interview_id}] Evaluation phase completed successfully. Final state: {self.state}")
        return True

    def generate_report(self):
        """Generates the PDF report for this session."""
        if not self.evaluation_complete:
             logger.warning(f"[{self.interview_id}] Evaluation must be complete before generating report. Attempting evaluation now.")
             # Trigger evaluation if not done
             if not self.perform_final_evaluation():
                 logger.error(f"[{self.interview_id}] Evaluation failed, cannot generate report.")
                 self._set_error_state("Failed to perform final evaluation, report cannot be generated.")
                 return None # Indicate report cannot be generated

        if self.report_generated and self.report_path and os.path.exists(self.report_path):
             logger.info(f"[{self.interview_id}] Report already generated: {self.report_path}")
             return self.report_path

        logger.info(f"[{self.interview_id}] Generating PDF report...")
        try:
            # Ensure report directory exists
            os.makedirs(config.REPORT_FOLDER, exist_ok=True)
            # Sanitize interview ID for filename if needed, although UUIDs are usually safe
            safe_interview_id = re.sub(r'[^\w\-]+', '_', self.interview_id)
            report_filename = config.REPORT_FILENAME_TEMPLATE.format(interview_id=safe_interview_id)
            output_path = os.path.join(config.REPORT_FOLDER, report_filename)

            # Call the report generator function from the dedicated module
            report_generator.generate_pdf_report(
                evaluated_data=self.interview_qna,
                resume_text=self.resume_text_raw, # Pass raw texts for inclusion if needed
                jd_text=self.jd_text_raw,
                role_title=self.role_title,
                candidate_name=config.CANDIDATE_NAME, # Pass other relevant info
                interviewer_name=config.INTERVIEWER_AI_NAME,
                company_name=config.COMPANY_NAME,
                interview_id=self.interview_id,
                report_filename=output_path
            )
            self.report_generated = True
            self.report_path = output_path
            logger.info(f"[{self.interview_id}] PDF report generated successfully: {output_path}")
            return output_path
        except ImportError as imp_err:
             logger.error(f"[{self.interview_id}] Report generation library error (e.g., ReportLab): {imp_err}. Cannot generate PDF report.")
             self._set_error_state(f"Report generation library error: {imp_err}")
             return None
        except Exception as report_err:
             logger.error(f"[{self.interview_id}] Failed to generate PDF report: {report_err}", exc_info=True)
             self._set_error_state(f"Failed to generate PDF report: {report_err}")
             return None

    def get_state(self):
        """Returns the current state of the interview session."""
        # Return state and any error message if applicable
        return {"state": self.state, "error": self.error_message}

    def get_qna_data(self):
         """Returns the evaluated Q&A data."""
         if not self.evaluation_complete:
              logger.warning(f"[{self.interview_id}] Requesting QnA data before evaluation is complete.")
         return self.interview_qna

    def get_full_conversation(self):
         """Returns the full conversation history."""
         return self.conversation_history

# modules/llm_interface.py
import google.generativeai as genai
import logging
import time
import re
import os # Added to potentially access API key if not passed directly

import config # Import the central config

logger = logging.getLogger(__name__)

# --- Global LLM Client (Lazy Initialization) ---
LLM_CLIENTS = {} # Dictionary to hold initialized models

# --- Initialize LLM Client ---
def initialize_llm(model_name):
    """Initializes the GenerativeModel for a specific model name if not already done."""
    global LLM_CLIENTS
    if model_name not in LLM_CLIENTS:
        logger.info(f"Initializing Gemini model: {model_name}")
        if not config.GOOGLE_API_KEY:
             logger.error("GOOGLE_API_KEY not found in config. Cannot initialize Gemini model.")
             raise ValueError("GOOGLE_API_KEY is not configured.")
        try:
            # Configure the API key
            genai.configure(api_key=config.GOOGLE_API_KEY)
            # Create the model instance
            model = genai.GenerativeModel(model_name)
            LLM_CLIENTS[model_name] = model
            logger.info(f"Gemini model '{model_name}' initialized successfully.")
            # Optional: Add a small test query to confirm initialization?
            # try:
            #     model.generate_content("test", generation_config={"max_output_tokens": 5})
            #     logger.info(f"Test query successful for {model_name}.")
            # except Exception as test_e:
            #     logger.error(f"Test query failed for {model_name}: {test_e}", exc_info=True)
            #     del LLM_CLIENTS[model_name] # Remove if test fails
            #     raise ConnectionError(f"Failed to connect or query model {model_name} after initialization.") from test_e

        except Exception as e:
            logger.error(f"Failed to initialize Gemini model '{model_name}': {e}", exc_info=True)
            # Don't add to LLM_CLIENTS if initialization fails
            raise ConnectionError(f"Failed to initialize Gemini model {model_name}.") from e
    return LLM_CLIENTS[model_name]

# --- Query LLM Function ---
def query_llm(prompt, model_name, max_tokens, temperature, retries=2, delay=5):
    """
    Sends a prompt to the specified Google Gemini model and returns the response.

    Args:
        prompt (str): The input prompt for the LLM.
        model_name (str): The name of the Gemini model to use (e.g., "gemini-1.5-flash-latest").
        max_tokens (int): The maximum number of tokens to generate.
        temperature (float): The sampling temperature for generation.
        retries (int): Number of times to retry on failure.
        delay (int): Delay in seconds between retries.

    Returns:
        str: The generated text content from the LLM, or an error message string starting with "Error:".
    """
    try:
        model = initialize_llm(model_name) # Get or initialize the model
    except (ValueError, ConnectionError) as init_err:
        logger.error(f"LLM Initialization Error for {model_name}: {init_err}")
        return f"Error: LLM Initialization Failed - {init_err}"

    # Configure generation parameters
    generation_config = genai.types.GenerationConfig(
        max_output_tokens=max_tokens,
        temperature=temperature
        # Add other parameters if needed (top_p, top_k, stop_sequences)
        # top_p=0.9,
        # top_k=40,
    )

    # Configure safety settings (adjust as needed)
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]

    logger.debug(f"Sending prompt to {model_name} (approx {len(prompt)} chars). Max Tokens: {max_tokens}, Temp: {temperature}")

    for attempt in range(retries + 1):
        try:
            response = model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
                # stream=False # Set to True for streaming responses if needed later
            )

            # --- Handle potential safety blocks or empty responses ---
            if not response.candidates:
                 # Check prompt feedback for blockage reason
                 block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else 'Unknown'
                 block_details = response.prompt_feedback.safety_ratings if response.prompt_feedback else 'No details'
                 logger.warning(f"LLM ({model_name}) response blocked. Reason: {block_reason}. Details: {block_details}. Prompt length: {len(prompt)} chars.")
                 # Return a specific error message for blocked content
                 # Shorten prompt in log/error message if it's too long
                 prompt_snippet = (prompt[:200] + '...') if len(prompt) > 200 else prompt
                 return f"Error: Response blocked due to safety settings (Reason: {block_reason}). Review prompt content near: '{prompt_snippet}'"

            # Check the first candidate for finish reason
            candidate = response.candidates[0]
            finish_reason = candidate.finish_reason.name if candidate.finish_reason else 'UNKNOWN'

            if finish_reason == "STOP": # Normal completion
                logger.debug(f"LLM ({model_name}) generated response successfully. Finish reason: {finish_reason}")
                return candidate.content.parts[0].text
            elif finish_reason == "MAX_TOKENS":
                 logger.warning(f"LLM ({model_name}) response truncated due to max_tokens ({max_tokens}). Consider increasing limit or refining prompt.")
                 return candidate.content.parts[0].text # Return truncated text
            elif finish_reason == "SAFETY":
                 safety_ratings = candidate.safety_ratings if candidate.safety_ratings else 'No details'
                 logger.warning(f"LLM ({model_name}) response generation stopped due to safety settings. Finish Reason: {finish_reason}. Details: {safety_ratings}")
                 return f"Error: Response generation stopped by safety settings (Reason: {finish_reason})."
            elif finish_reason == "RECITATION":
                 logger.warning(f"LLM ({model_name}) response generation stopped due to recitation concerns. Finish Reason: {finish_reason}.")
                 return f"Error: Response generation stopped due to recitation concerns (Reason: {finish_reason})."
            else: # OTHER, UNKNOWN, etc.
                 logger.warning(f"LLM ({model_name}) response generation finished with unexpected reason: {finish_reason}. Response text (if any): {candidate.content.parts[0].text[:100] if candidate.content.parts else 'N/A'}...")
                 # Return text if available, otherwise indicate an issue
                 if candidate.content and candidate.content.parts:
                      return candidate.content.parts[0].text
                 else:
                      return f"Error: Response generation finished unexpectedly (Reason: {finish_reason}). No content returned."

        except Exception as e:
            logger.error(f"Error querying LLM ({model_name}) on attempt {attempt + 1}/{retries + 1}: {e}", exc_info=True)
            if attempt < retries:
                logger.info(f"Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                # Check for common API errors in the exception message
                error_str = str(e)
                if "API key not valid" in error_str:
                     return "Error: Invalid Google API Key. Please check your configuration."
                elif "quota" in error_str.lower():
                     return f"Error: API quota exceeded for model {model_name}. Please check your Google Cloud project limits."
                elif "resource_exhausted" in error_str.lower():
                      return f"Error: Resource exhausted for model {model_name}. The service might be temporarily overloaded. Please try again later."
                # Generic error if specific checks fail
                return f"Error: Failed to query LLM {model_name} after {retries + 1} attempts. Last error: {e}"

    # Should not be reachable if loop completes, but added for safety
    return f"Error: LLM query failed for {model_name} after retries."


# --- Clean LLM Output Function ---
def clean_llm_output(raw_text, is_evaluation=False):
    """
    Cleans the raw text output from the LLM.
    Removes common artifacts like markdown formatting characters,
    leading/trailing whitespace, and potentially unwanted preamble/postamble.
    """
    if raw_text is None:
        return ""
    if not isinstance(raw_text, str):
         logger.warning(f"clean_llm_output received non-string input: {type(raw_text)}. Returning empty string.")
         return ""

    text = raw_text

    # Remove common markdown code block fences and language identifiers
    text = re.sub(r"```[\w\s]*\n", "", text)
    text = re.sub(r"```", "", text)

    # Remove common markdown emphasis/strong markers if they wrap the entire response or parts
    # Be cautious not to remove them if they are part of legitimate content (e.g., code examples)
    # This is a simple removal, might need refinement
    text = text.replace("**", "").replace("__", "")
    text = text.replace("*", "").replace("_", "") # More aggressive removal

    # Remove leading/trailing whitespace
    text = text.strip()

    # Optional: Remove common conversational filler if not desired in final output
    # (Be careful with this, might remove intended conversational style)
    # common_fillers = ["Okay, ", "Alright, ", "Sure, ", "Certainly, ", "Here is ", "Here's "]
    # for filler in common_fillers:
    #     if text.lower().startswith(filler.lower()):
    #         text = text[len(filler):]

    # Specific cleaning for evaluations (remove preamble if model adds it)
    if is_evaluation:
         # Find the start of the structured evaluation part
         start_keywords = ["Evaluation:", "Relevance & Understanding:"]
         start_index = -1
         for keyword in start_keywords:
              try:
                   idx = text.index(keyword)
                   if start_index == -1 or idx < start_index:
                        start_index = idx
              except ValueError:
                   continue # Keyword not found

         if start_index > 0: # If keyword found and it's not at the very beginning
              # Check if the text before the keyword is just short filler/noise
              preamble = text[:start_index].strip()
              if len(preamble) < 50 and '\n' not in preamble: # Heuristic for short preamble
                   logger.debug(f"Removing potential evaluation preamble: '{preamble}'")
                   text = text[start_index:]
         elif start_index == -1:
              logger.warning("Could not find standard evaluation start keywords. Using raw cleaned text.")

    # Final strip just in case
    text = text.strip()

    return text

# --- Optional: Function to initialize all configured models at once ---
def initialize_llms():
    """Initializes all LLM models defined in the config."""
    logger.info("Initializing all configured LLMs...")
    models_to_init = [
        config.INTERVIEWER_LLM_MODEL_NAME,
        config.EVALUATOR_LLM_MODEL_NAME
    ]
    initialized_count = 0
    for model_name in set(models_to_init): # Use set to avoid duplicates
        if model_name: # Check if model name is defined
            try:
                initialize_llm(model_name)
                initialized_count += 1
            except Exception as e:
                 logger.error(f"Failed to initialize {model_name} during bulk init: {e}")
        else:
             logger.warning("Skipping initialization for an undefined LLM model name in config.")
    logger.info(f"LLM bulk initialization complete. {initialized_count} models ready.")

# prompt_templates.py
from config import NUM_QUESTIONS # Import constant from config file

# --- Prompt for Initial Question Generation ---
# This prompt guides the LLM to create a set of interview questions,
# including technical/scenario-based ones and one focused on past projects.
# prompt_templates.py

# --- Prompt for Initial Question Generation ---
QUESTION_GENERATION_PROMPT_TEMPLATE = """
You are an expert technical interviewer preparing questions for a candidate applying for a **{role_title}** role. Your goal is to assess their suitability by bridging their background with the specific job requirements, focusing on **problem-solving ability, depth of understanding, and practical application** through scenario-based questions and understanding their past work.

**Candidate Background (Summary from Resume):**
{resume_summary}...
[End of Resume Summary]

**Candidate Project/Experience Details (Extracted from Resume):**
{project_details}
[End Project Details]

**Target Role Requirements (Summary from Job Description):**
{jd_summary}...
[End of Job Description Summary]

**Key Focus Topics (Identified from Resume & JD Analysis):**
{focus_str}

**Retrieved Context (Relevant information from Knowledge Base):**
{context_str}
[End of Retrieved Context]

**Instructions:**
Based *only* on the information provided above, generate **exactly {num_questions} diverse, high-quality technical/scenario-based questions PLUS one insightful question specifically about the candidate's past projects/experience** (using the 'Candidate Project/Experience Details' section). This means a total of {num_questions_plus_one} questions. Prioritize quality and depth, BUT **ensure each numbered question is concise and focuses on a single, specific point or task.** Complex scenarios should be broken down across *multiple potential turns* (you generate the starting question here, follow-ups happen later).

**Technical/Scenario Questions ({num_questions} required):**
1.  Directly relate to the **Key Focus Topics** and the **Target Role Requirements**.
2.  **Emphasize scenarios BUT keep them focused:** Frame questions around realistic situations, problems, or tasks relevant to the role (e.g., "Imagine you need to...", "Given this situation...", "How would you approach..."). **Avoid asking multiple unrelated things in one question.** For example, instead of asking about DB choice *and* API design *and* scaling in one go, ask *just* about the initial DB choice trade-offs.
3.  Require **synthesis and critical thinking**, integrating knowledge from the candidate's background, job requirements, and the provided context (when relevant).
4.  Include a *diverse mix* of question types relevant to the role, using the specific tags provided below (do not include the tags in the final question text):
    *   **Conceptual/Application questions** ({role_specific_guidance}).
    *   **Coding/Query questions** set within a practical context ({coding_guidance}) - **keep the required code snippet short and focused.**
    *   **Design/Problem-solving/Troubleshooting questions** presented as scenarios ({problem_solving_guidance}) - **focus on one specific problem per question.**
    *   **Behavioral questions** framed around specific past experiences or hypothetical situations related to the role's demands (e.g., learning, teamwork, handling challenges).
    {extra_hints_str}
5.  Be specific to the **{role_title} role** and level. Ensure complexity is appropriate.
6.  Do not invent information. Base questions strictly on the provided summaries and context.
7.  **Keep each numbered question relatively short and easy to grasp for a verbal response.** The goal is to initiate a topic, not exhaust it in one question.

**Project/Experience Specific Question (1 required):**
*   Ask the candidate to elaborate on a specific project or experience mentioned in their details. **Start with an open-ended but focused prompt,** like asking about their *primary role* or the *main goal* of the project. You can probe deeper on challenges/learnings in follow-up turns. (Use Tag: [Project Deep Dive])

**Generate {num_questions_plus_one} CONCISE Interview Questions. Format as a numbered list. Do not include the tags like '[DB Concept]' or '[Project Deep Dive]' in the output question text itself.**
1. {q_type_0} ...
2. {q_type_1} ...
3. {q_type_2} ...
4. {q_type_3} ...
5. {q_type_4} ...
6. {q_type_5} ...
7. [Project Deep Dive] ... (LLM generates appropriate focused project question based on details)
"""

# --- Prompt for Conversational Interview Turn ---
# This prompt guides the AI interviewer on how to behave during the conversation,
# including when to ask prepared questions vs. follow-ups, and how to format
# the output for Text-to-Speech (TTS) clarity.
CONVERSATIONAL_INTERVIEW_PROMPT_TEMPLATE = """
**SYSTEM PROMPT**

You are **{interviewer_name}**, an AI Interviewer from **{company_name}**, conducting a technical and project-focused interview for the **{role_title}** role with **{candidate_name}**.

Your goal is to have a natural, professional, and encouraging conversation. You need to skillfully weave together prepared questions with **relevant, probing follow-up questions** to gain deeper insights. **Aim to ask 1-2 meaningful follow-ups when appropriate before moving to the next prepared question.** Your spoken output will be converted to speech using Text-to-Speech (TTS). Please format your responses accordingly.

**Background Information (For Your Reference):**

*   Candidate Resume Summary: {resume_summary}
*   Candidate Project/Experience Details: {project_details}
*   Job Description Summary: {jd_summary}
*   Key Focus Topics: {focus_topics_str}
*   Full List of Prepared Questions (Includes a project question):
{prepared_questions_numbered}

**Interview State:**

*   Prepared Questions Asked So Far (Indices): {asked_questions_str}
*   Prepared Questions Remaining (Indices): {remaining_questions_str}

---
**CONVERSATION HISTORY (Most Recent Turns First)**
{conversation_history}
---

**YOUR TURN, {interviewer_name}:**

Your task is to analyze the candidate's last response and decide whether to ask a **targeted follow-up question** or proceed to the next **prepared question**.

1.  **Review Last Response:** Carefully consider the candidate's most recent answer in the CONVERSATION HISTORY. Assess its clarity, depth, and completeness relative to the question asked.

2.  **Decide: Follow-up or Next Prepared Question?**
    *   **PRIORITIZE Asking a Follow-up (Aim for 1-2 before moving on) IF:**
        *   The previous answer was insightful but mentioned a specific technology, trade-off, challenge, or result that could be explored further (e.g., "You mentioned using X. Why was that chosen over Y?", "What specific metrics showed success for that approach?").
        *   OR The previous technical explanation was correct but high-level, and asking for a concrete example, a 'why', or a specific implementation detail would reveal deeper understanding.
        *   OR The previous answer was about a project/experience and lacked specific details on *their* contribution, *specific* technical hurdles, *quantifiable* outcomes, or *concrete* learnings.
        *   OR The previous answer was somewhat vague, potentially inaccurate, or could be significantly clarified by asking for elaboration on a *specific part* of their response.
        *   OR The previous answer was extremely short (e.g., "I don't know", "Yes") and asking for elaboration or an alternative approach seems appropriate.

    *   **Follow-up Question Guidance:**
        *   If asking a follow-up, make it **concise, specific, and directly tied** to something the candidate *just said*. Your goal is **probing for depth or clarification**.
        *   Use clear language suitable for TTS.
        *   **Good examples:** "Could you elaborate on the specific database optimization technique you used there?", "You mentioned scalability challenges – what was the main bottleneck you encountered?", "What data structure did you use for X and why was it suitable?", "Can you give a brief code example of how you handled that error condition?".
        *   **AVOID generic follow-ups** like "Can you tell me more?" or "Why?". Instead, ask "Why *specifically* did you choose..." or "Tell me more about *the performance aspect*...".
        *   **Ask only ONE follow-up question at a time.**

    *   **Ask the Next Prepared Question IF:**
        *   The candidate's answer was thorough and complete for the asked question, and there isn't a clear, high-value point to probe deeper on immediately.
        *   OR You have already asked **one or two** relevant follow-up questions related to the *previous* prepared question/topic. (It's time to move the interview forward).

3.  **Formulate Your Response (TTS Friendly):**
    *   **Clarity:** Use clear, standard English. Avoid complex sentences. Use complete sentences.
    *   **Pronunciation:** (Same guidance as before - avoid symbols, spell out if needed, use punctuation for TTS pacing).
    *   **If Asking Follow-up:** Directly ask your concise, TTS-friendly follow-up question. You might use a very brief connector like "Okay, and on that point..." or just ask the question directly.
    *   **If Asking Next Prepared Question:**
        *   Provide a *brief, natural* transition acknowledging the previous answer *or follow-up exchange* (e.g., "Alright, thanks for clarifying that.", "Understood.", "Interesting perspective."). Vary your transitions slightly.
        *   Select the *next available question index* from the 'Prepared Questions Remaining' list.
        *   Clearly state the selected prepared question using TTS-friendly language. (Remember these prepared questions should now be shorter due to the changes in the generation prompt).

4.  **Tone:** Maintain a professional, friendly, and encouraging tone throughout.

**Output ONLY your response as {interviewer_name}. Do NOT include meta-commentary, your reasoning, bracketed notes, or any text other than what you would say clearly to the candidate for TTS conversion.**
"""

# --- Prompt for Evaluation ---
# This prompt guides the LLM to evaluate a single question-answer pair
# based on provided criteria and context.
EVALUATION_PROMPT_TEMPLATE = """
**SYSTEM PROMPT**

You are an expert Technical Interview Evaluator. Your task is to assess the candidate's response to a specific interview question based on their background (resume), the requirements of the target role (job description), and the technical correctness or relevance of their answer. Be objective and provide constructive feedback.

**Input Information:**

**1. Role Title:** {role_title}
**2. Job Description Summary:**
{jd_summary}
[End Job Description Summary]

**3. Candidate Resume Summary:**
{resume_summary}
[End Resume Summary]

**4. Interview Question Asked:**
"{interview_question}"

**5. Candidate's Response:**
"{candidate_response}"

**Evaluation Task:**

Provide a concise evaluation of the candidate's response based *only* on the information provided. Structure your evaluation clearly using the following sections. Be specific in your feedback.

*   **Alignment with Question:** Did the candidate directly address all parts of the question asked? Was the answer relevant to the core topic? (Briefly state Yes/No/Partially and explain succinctly).
*   **Technical Accuracy/Conceptual Understanding:** Was the technical information provided correct? Did the candidate demonstrate an appropriate depth of understanding for the targeted role level? (Assess correctness and depth relative to the role).
*   **Relevance to Role/Resume:** Does the answer demonstrate skills, knowledge, or problem-solving approaches relevant to the **{role_title}** role as described in the JD? Does it connect with or appropriately leverage experience mentioned in the resume summary?
*   **Clarity and Structure:** Was the response well-organized and easy to follow? Did the candidate articulate their thoughts clearly and concisely?
*   **Strengths:** List 1-2 key strengths demonstrated *specifically* in this response (e.g., clear explanation of X, practical example provided for Y, relevant experience Z cited, logical problem-solving approach). Be specific.
*   **Areas for Improvement:** List 1-2 specific, actionable areas where *this response* could be improved (e.g., lacked detail on X aspect, could have mentioned Y technology/concept, minor inaccuracy regarding Z, explanation of reasoning could be clearer). Be constructive.
*   **Overall Score (1-5):** Assign a numerical score reflecting the quality of this specific answer relative to expectations for a candidate applying for this role (1=Poor, 2=Weak, 3=Average, 4=Good, 5=Excellent).
*   **Justification:** Provide a brief (1-2 sentence) justification for the assigned score, summarizing the key factors from the points above.

**Output only the structured evaluation using the specified headings. Do not add extra introductory or concluding remarks.**

**Evaluation:**

*   **Alignment with Question:** ...
*   **Technical Accuracy/Conceptual Understanding:** ...
*   **Relevance to Role/Resume:** ...
*   **Clarity and Structure:** ...
*   **Strengths:**
    *   ...
*   **Areas for Improvement:**
    *   ...
*   **Overall Score (1-5):** ...
*   **Justification:** ...
"""

# modules/report_generator.py
import logging
import os
from datetime import datetime
import re
# Attempt to import ReportLab
try:
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib import colors
    from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER, TA_RIGHT
    from reportlab.lib.units import inch
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False
    logging.getLogger(__name__).error("ReportLab library not found. Please install it (`pip install reportlab`) to generate PDF reports.")
    # Define dummy classes/functions if ReportLab is not available to avoid NameErrors later
    # Although the generate_pdf_report function will check REPORTLAB_AVAILABLE first.
    letter = None
    SimpleDocTemplate = None
    Paragraph = None
    Spacer = None
    Table = None
    TableStyle = None
    PageBreak = None
    getSampleStyleSheet = None
    ParagraphStyle = None
    colors = None
    TA_JUSTIFY = TA_LEFT = TA_CENTER = TA_RIGHT = None
    inch = None

# Local Imports
import config

logger = logging.getLogger(__name__)

# --- Report Generation ---

def generate_pdf_report(evaluated_data, resume_text, jd_text, role_title,
                        candidate_name, interviewer_name, company_name, interview_id,
                        report_filename):
    """
    Generates a PDF interview report using ReportLab.

    Args:
        evaluated_data (list): List of dictionaries, where each dict represents
                               a Q&A turn with evaluation results. Expected keys:
                               'question_turn', 'question', 'response',
                               'evaluation', 'score', 'score_justification',
                               'confidence_score', 'confidence_rating',
                               'primary_emotion', 'confidence_analysis_error',
                               'confidence_message', 'stt_success', 'stt_error_message'.
        resume_text (str): Raw text extracted from the candidate's resume.
        jd_text (str): Raw text from the job description.
        role_title (str): The job title for the interview.
        candidate_name (str): Name of the candidate.
        interviewer_name (str): Name of the AI interviewer.
        company_name (str): Name of the company.
        interview_id (str): Unique ID for the interview session.
        report_filename (str): The full path where the PDF report should be saved.

    Returns:
        bool: True if the report was generated successfully, False otherwise.
    """
    if not REPORTLAB_AVAILABLE:
        logger.error("ReportLab not installed. Cannot generate PDF report.")
        return False

    logger.info(f"Generating PDF report for Interview ID {interview_id} to {report_filename}...")

    try:
        doc = SimpleDocTemplate(report_filename, pagesize=letter,
                                topMargin=0.75 * inch, bottomMargin=0.75 * inch,
                                leftMargin=0.75 * inch, rightMargin=0.75 * inch)
        styles = getSampleStyleSheet()
        story = []

        # --- Custom Styles ---
        # Title Style
        title_style = ParagraphStyle(name='TitleStyle', parent=styles['h1'], alignment=TA_CENTER, spaceAfter=20, fontSize=18)
        # Subtitle Style
        subtitle_style = ParagraphStyle(name='SubtitleStyle', parent=styles['h2'], alignment=TA_CENTER, spaceAfter=12, fontSize=14, textColor=colors.darkblue)
        # Section Header Style
        section_header_style = ParagraphStyle(name='SectionHeader', parent=styles['h2'], spaceBefore=18, spaceAfter=10, fontSize=13, textColor=colors.darkslategray)
        # Normal Text Style (Justified)
        normal_justified = ParagraphStyle(name='NormalJustified', parent=styles['Normal'], alignment=TA_JUSTIFY, spaceAfter=6)
         # Code/Preformatted Text Style
        code_style = ParagraphStyle(name='CodeStyle', parent=styles['Code'], fontSize=9, leading=11, spaceAfter=10, leftIndent=10, rightIndent=10, backColor=colors.whitesmoke, borderPadding=5)
        # Question/Answer Labels
        q_style = ParagraphStyle(name='QuestionLabel', parent=styles['Normal'], spaceAfter=2, textColor=colors.darkred, fontName='Helvetica-Bold')
        a_style = ParagraphStyle(name='AnswerLabel', parent=styles['Normal'], spaceAfter=2, textColor=colors.darkgreen, fontName='Helvetica-Bold')
        eval_style = ParagraphStyle(name='EvalLabel', parent=styles['Normal'], spaceAfter=2, textColor=colors.darkblue, fontName='Helvetica-Bold')
        # Confidence Labels
        conf_style = ParagraphStyle(name='ConfLabel', parent=styles['Italic'], spaceAfter=2, textColor=colors.dimgray, fontSize=9)


        # --- Report Header ---
        story.append(Paragraph(f"{company_name} - Interview Report", title_style))
        story.append(Paragraph(f"Role: {role_title}", subtitle_style))
        story.append(Spacer(1, 0.2 * inch))
        header_data = [
            ['Candidate:', candidate_name, 'Date:', datetime.now().strftime("%Y-%m-%d %H:%M")],
            ['Interviewer:', interviewer_name, 'Interview ID:', interview_id[-12:]], # Show last part of ID
        ]
        header_table = Table(header_data, colWidths=[1.2 * inch, 3 * inch, 1 * inch, 2 * inch])
        header_table.setStyle(TableStyle([
            ('ALIGN', (0, 0), (0, -1), 'RIGHT'),
            ('ALIGN', (2, 0), (2, -1), 'RIGHT'),
            ('ALIGN', (1, 0), (1, -1), 'LEFT'),
            ('ALIGN', (3, 0), (3, -1), 'LEFT'),
            ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
            ('FONTNAME', (2, 0), (2, -1), 'Helvetica-Bold'),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        ]))
        story.append(header_table)
        story.append(Spacer(1, 0.3 * inch))

        # --- Overall Summary Section (Calculate from evaluated data) ---
        story.append(Paragraph("Overall Performance Summary", section_header_style))
        total_score = 0
        valid_scores = 0
        valid_confidence = 0
        total_confidence = 0.0
        key_strengths = []
        key_areas_for_improvement = []

        for item in evaluated_data:
            if item.get("score") is not None:
                total_score += item["score"]
                valid_scores += 1
            if item.get("confidence_score") is not None and not item.get("confidence_analysis_error"):
                 total_confidence += item["confidence_score"]
                 valid_confidence += 1
            # Simple aggregation of strengths/weaknesses (can be improved with LLM summary later)
            eval_text = item.get("evaluation", "")
            if eval_text:
                strengths_match = re.search(r"Strengths:\s*(.*?)(?:Areas for Improvement:|Overall Score:|\Z)", eval_text, re.DOTALL | re.IGNORECASE)
                if strengths_match: key_strengths.extend(s.strip() for s in strengths_match.group(1).strip().split('*') if s.strip())
                areas_match = re.search(r"Areas for Improvement:\s*(.*?)(?:Overall Score:|\Z)", eval_text, re.DOTALL | re.IGNORECASE)
                if areas_match: key_areas_for_improvement.extend(a.strip() for a in areas_match.group(1).strip().split('*') if a.strip())

        # Deduplicate and limit summary points
        key_strengths = list(set(s for s in key_strengths if len(s) > 5))[:3]
        key_areas_for_improvement = list(set(a for a in key_areas_for_improvement if len(a) > 5))[:3]


        avg_score = total_score / valid_scores if valid_scores > 0 else "N/A"
        avg_conf = (total_confidence / valid_confidence * 100) if valid_confidence > 0 else "N/A" # Assuming score is 0-1 scale

        summary_text = f"Average Score: {avg_score:.2f}/5.0 (based on {valid_scores} evaluated responses)<br/>" if isinstance(avg_score, float) else f"Average Score: {avg_score} (no valid scores)<br/>"
        summary_text += f"Average Confidence: {avg_conf:.1f}% (based on {valid_confidence} analyzed responses)<br/>" if isinstance(avg_conf, float) else f"Average Confidence: {avg_conf} (no valid analyses)<br/>"

        if key_strengths:
             summary_text += "<br/><b>Potential Strengths Observed:</b><br/>" + "<br/>".join(f"- {s}" for s in key_strengths)
        if key_areas_for_improvement:
             summary_text += "<br/><br/><b>Potential Areas for Development:</b><br/>" + "<br/>".join(f"- {s}" for s in key_areas_for_improvement)

        story.append(Paragraph(summary_text, normal_justified))
        story.append(Spacer(1, 0.3 * inch))


        # --- Detailed Q&A Section ---
        story.append(Paragraph("Detailed Question & Answer Evaluation", section_header_style))

        for i, item in enumerate(evaluated_data):
            turn = item.get("question_turn", i + 1)
            question = item.get("question", "N/A")
            response = item.get("response", "N/A")
            evaluation = item.get("evaluation", "N/A")
            score = item.get("score", "N/A")
            justification = item.get("score_justification", "N/A")

            # Confidence/Emotion data
            conf_score = item.get('confidence_score')
            conf_rating = item.get('confidence_rating', 'N/A')
            conf_emotion = item.get('primary_emotion', 'N/A')
            conf_error = item.get('confidence_analysis_error', False)
            conf_message = item.get('confidence_message', '')

            stt_success = item.get('stt_success', True) # Assume success if key missing (legacy)
            stt_error_msg = item.get('stt_error_message', None)

            # Format confidence string
            conf_str = ""
            if not conf_error and conf_score is not None:
                conf_str = f"Confidence: {conf_score*100:.1f}% ({conf_rating}), Primary Emotion: {conf_emotion}"
            elif conf_error:
                conf_str = f"Confidence Analysis Note: {conf_message}"
            elif not stt_success:
                 conf_str = f"Confidence Analysis Skipped: {stt_error_msg or 'STT Failed'}"
            elif response == "[Audio detected - No speech recognized]":
                 conf_str = f"Confidence Analysis Skipped: No speech detected"


            story.append(Paragraph(f"<u>Turn {turn}: Question</u>", q_style))
            story.append(Paragraph(question, normal_justified))
            story.append(Spacer(1, 0.05 * inch))

            story.append(Paragraph("Candidate Response:", a_style))
            # Handle STT errors explicitly in the report
            if not stt_success:
                 story.append(Paragraph(f"<i>[STT Error: {stt_error_msg}]</i>", ParagraphStyle(name='ErrorStyle', parent=styles['Italic'], textColor=colors.red)))
            elif response == "[Audio detected - No speech recognized]":
                 story.append(Paragraph("<i>[No speech recognized in audio]</i>", styles['Italic']))
            else:
                 story.append(Paragraph(response, normal_justified))

            # Add confidence info if available
            if conf_str:
                 story.append(Paragraph(conf_str, conf_style))

            story.append(Spacer(1, 0.1 * inch))

            # Add Evaluation if STT was successful and speech was present
            if stt_success and response != "[Audio detected - No speech recognized]":
                story.append(Paragraph("Evaluation:", eval_style))
                # Check if evaluation itself had an error
                if evaluation.startswith("Evaluation Error:") or evaluation.startswith("Evaluation skipped"):
                     story.append(Paragraph(f"<i>[{evaluation}]</i>", ParagraphStyle(name='EvalErrorStyle', parent=styles['Italic'], textColor=colors.orange)))
                else:
                     # Display full evaluation, preserving line breaks from LLM output
                     eval_paragraph = Paragraph(evaluation.replace('\n', '<br/>'), normal_justified)
                     story.append(eval_paragraph)
                     # story.append(Paragraph(f"<b>Score (1-5): {score}</b>", normal_justified))
                     # story.append(Paragraph(f"Justification: {justification}", normal_justified))
            else:
                 # If STT failed or no speech, indicate no text evaluation performed
                 story.append(Paragraph("Evaluation:", eval_style))
                 reason = stt_error_msg or "No speech detected"
                 story.append(Paragraph(f"<i>[Text evaluation skipped due to: {reason}]</i>", styles['Italic']))


            story.append(Spacer(1, 0.25 * inch)) # Space between Q&A blocks

        # --- Appendix: Resume and JD Text (Optional) ---
        add_appendix = True # Control whether to add this section
        if add_appendix:
            story.append(PageBreak())
            story.append(Paragraph("Appendix A: Candidate Resume Text", section_header_style))
            # Use code style for better readability of potentially messy text
            resume_paragraph = Paragraph(resume_text.replace('\n', '<br/>'), code_style)
            story.append(resume_paragraph)

            story.append(PageBreak())
            story.append(Paragraph("Appendix B: Job Description Text", section_header_style))
            jd_paragraph = Paragraph(jd_text.replace('\n', '<br/>'), code_style)
            story.append(jd_paragraph)

        # --- Build the PDF ---
        doc.build(story)
        logger.info(f"PDF report saved successfully to {report_filename}")
        return True

    except ImportError:
        logger.error("ReportLab Error: Could not generate report due to missing library.", exc_info=True)
        return False # Should have been caught earlier, but double-check
    except Exception as e:
        logger.error(f"Failed to generate PDF report {report_filename}: {e}", exc_info=True)
        return False

# --- Regex Helper ---
# Moved the regex logic to the summary calculation part above
# import re # Already imported if needed elsewhere

# modules/utils.py
import logging
import os
import re
import warnings

# PDF Parsing
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logging.getLogger(__name__).warning("pdfplumber not found. PDF text extraction will fail. Install with: pip install pdfplumber")

# NLTK for text processing
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk import pos_tag
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    stopwords = None
    word_tokenize = None
    sent_tokenize = None
    pos_tag = None
    logging.getLogger(__name__).warning("NLTK not found. Text processing features (keyword extraction, summarization hints) may be limited. Install with: pip install nltk")

# RAG Dependencies (Optional, based on config)
try:
    import psycopg2 # For PostgreSQL connection
    from sentence_transformers import SentenceTransformer # For embeddings
    RAG_DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    RAG_DEPENDENCIES_AVAILABLE = False
    psycopg2 = None
    SentenceTransformer = None
    logging.getLogger(__name__).warning(f"RAG dependencies not fully met ({e}). RAG features might be disabled. Install: pip install psycopg2-binary sentence-transformers torch")


# Local Imports
import config

logger = logging.getLogger(__name__)

# --- Global Variables for RAG ---
db_connection = None
db_cursor = None
embedding_model = None
nltk_initialized = False


# --- NLTK Initialization ---
def initialize_nltk():
    """Downloads necessary NLTK data if not already present."""
    global nltk_initialized
    if not NLTK_AVAILABLE or nltk_initialized:
        return

    required_data = ["punkt", "stopwords", "averaged_perceptron_tagger"]
    logger.info("Checking NLTK data...")
    try:
        for package in required_data:
            try:
                # Check if data exists, e.g., nltk.data.find(f'tokenizers/{package}')
                # Simpler approach: just attempt download, it handles existing data gracefully.
                logger.debug(f"Ensuring NLTK package '{package}' is available...")
                nltk.download(package, quiet=True) # quiet=True suppresses console output unless error
            except Exception as e:
                # This might happen due to network issues or permissions
                 logger.error(f"Failed to download NLTK data '{package}': {e}. Some text processing features might fail.", exc_info=True)
                 # Optionally raise an error or warn more severely
                 # raise RuntimeError(f"Failed to initialize NLTK data '{package}'") from e

        logger.info("NLTK data is available.")
        nltk_initialized = True
    except AttributeError:
        # nltk might be None if import failed
        logger.error("NLTK library not available. Cannot initialize data.")
    except Exception as e:
        logger.error(f"An unexpected error occurred during NLTK initialization: {e}", exc_info=True)

# --- RAG Initialization ---
def initialize_rag():
    """Initializes embedding model and database connection for RAG."""
    global embedding_model, db_connection, db_cursor

    if config.RETRIEVAL_TOP_K <= 0:
        logger.info("RAG is disabled (RETRIEVAL_TOP_K <= 0). Skipping initialization.")
        return

    if not RAG_DEPENDENCIES_AVAILABLE:
        logger.warning("RAG dependencies not met. RAG features will be disabled.")
        config.RETRIEVAL_TOP_K = 0 # Disable RAG if deps missing
        return

    # 1. Initialize Embedding Model
    if embedding_model is None:
        try:
            logger.info(f"Loading embedding model: {config.EMBEDDING_MODEL_NAME}")
            # Specify device if CUDA is available and desired? device='cuda'
            embedding_model = SentenceTransformer(config.EMBEDDING_MODEL_NAME)
            logger.info("Embedding model loaded successfully.")
            # Optional: Test embedding
            # test_embedding = embedding_model.encode("test sentence")
            # logger.debug(f"Test embedding generated (shape: {test_embedding.shape})")
        except Exception as e:
            logger.error(f"Failed to load embedding model '{config.EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
            embedding_model = None # Ensure it remains None on failure
            config.RETRIEVAL_TOP_K = 0 # Disable RAG if model fails
            logger.warning("Disabling RAG due to embedding model failure.")
            return # Stop initialization here

    # 2. Initialize Database Connection
    if db_connection is None:
        if not all([config.DB_NAME, config.DB_USER, config.DB_HOST]):
             logger.error("Database configuration (DB_NAME, DB_USER, DB_HOST) is incomplete. Cannot connect to RAG database.")
             config.RETRIEVAL_TOP_K = 0 # Disable RAG
             logger.warning("Disabling RAG due to incomplete DB configuration.")
             return

        try:
            logger.info(f"Connecting to RAG database '{config.DB_NAME}' on {config.DB_HOST}:{config.DB_PORT} as user '{config.DB_USER}'...")
            db_connection = psycopg2.connect(
                dbname=config.DB_NAME,
                user=config.DB_USER,
                password=config.DB_PASSWORD,
                host=config.DB_HOST,
                port=config.DB_PORT,
                connect_timeout=10 # Add a connection timeout
            )
            db_cursor = db_connection.cursor()
            # Test connection with a simple query
            db_cursor.execute("SELECT version();")
            db_version = db_cursor.fetchone()
            logger.info(f"Database connection successful. PostgreSQL version: {db_version[0]}")
        except psycopg2.OperationalError as e:
            logger.error(f"Failed to connect to database: {e}", exc_info=True)
            db_connection = None
            db_cursor = None
            config.RETRIEVAL_TOP_K = 0 # Disable RAG
            logger.warning("Disabling RAG due to database connection failure.")
        except Exception as e:
             logger.error(f"An unexpected error occurred during database connection: {e}", exc_info=True)
             db_connection = None
             db_cursor = None
             config.RETRIEVAL_TOP_K = 0 # Disable RAG
             logger.warning("Disabling RAG due to unexpected database connection error.")

# Call initializers immediately or rely on before_first_request in app.py
# initialize_nltk()
# initialize_rag() # Let app.py handle initialization

# --- PDF Text Extraction ---
def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file using pdfplumber."""
    if not PDFPLUMBER_AVAILABLE:
        logger.error("pdfplumber library is not available. Cannot extract text from PDF.")
        return None # Return None or raise an error

    if not os.path.exists(pdf_path):
        logger.error(f"PDF file not found at path: {pdf_path}")
        return None

    logger.info(f"Extracting text from PDF: {os.path.basename(pdf_path)}")
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            num_pages = len(pdf.pages)
            logger.debug(f"PDF has {num_pages} pages.")
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
                # else:
                #     logger.warning(f"No text extracted from page {i+1} of {os.path.basename(pdf_path)}. Might be image-based.")
            logger.info(f"Successfully extracted {len(text)} characters from PDF.")
            return text.strip() # Return stripped text
    except Exception as e:
        logger.error(f"Error extracting text from PDF '{os.path.basename(pdf_path)}': {e}", exc_info=True)
        return None # Return None on error


# --- Text Cleaning ---
def clean_text(text):
    """Basic text cleaning: remove extra whitespace, normalize line breaks."""
    if not isinstance(text, str): return ""
    text = re.sub(r'\s+', ' ', text) # Replace multiple whitespace chars with single space
    text = re.sub(r'\n+', '\n', text) # Normalize line breaks
    return text.strip()

# --- NLTK Based Keyword Extraction (Example) ---
def extract_keywords(text, max_keywords=10):
    """Extracts potential keywords using NLTK POS tagging (simple approach)."""
    if not NLTK_AVAILABLE or not nltk_initialized:
        logger.warning("NLTK not available or initialized. Skipping keyword extraction.")
        return []
    if not text: return []

    try:
        # Tokenize and POS tag
        words = word_tokenize(text.lower())
        tagged_words = pos_tag(words)

        # Filter for nouns (NN, NNS, NNP, NNPS) and potentially adjectives (JJ)
        stop_words = set(stopwords.words('english'))
        keywords = [word for word, tag in tagged_words if tag.startswith('NN') and word.isalnum() and word not in stop_words and len(word) > 2]

        # Get frequency distribution
        freq_dist = nltk.FreqDist(keywords)

        # Return most common keywords
        return [kw for kw, freq in freq_dist.most_common(max_keywords)]
    except Exception as e:
        logger.error(f"Error during NLTK keyword extraction: {e}", exc_info=True)
        return []


# --- RAG Helper Functions ---

def generate_search_queries(resume_summary, jd_summary, num_queries=3):
    """Generates potential search queries for RAG based on resume/JD."""
    # Simple approach: Extract keywords/keyphrases from both and combine
    # More advanced: Use an LLM to generate specific questions based on the role/candidate
    logger.debug("Generating RAG search queries...")
    if not NLTK_AVAILABLE or not nltk_initialized:
         logger.warning("NLTK unavailable, using basic combined text for RAG query.")
         # Fallback: just use combined text as a single query
         combined_text = f"{role_title}: {jd_summary} Candidate skills: {resume_summary}"
         return [clean_text(combined_text)[:500]] # Limit length

    try:
        role_title_match = re.search(r"^(?:Job\s+)?Title\s*[:\-]?\s*(.*?)(\n|$)", jd_summary, re.IGNORECASE | re.MULTILINE)
        role_title = role_title_match.group(1).strip() if role_title_match else "Position"

        resume_keywords = extract_keywords(resume_summary, max_keywords=8)
        jd_keywords = extract_keywords(jd_summary, max_keywords=8)

        # Combine unique keywords
        combined_keywords = list(set(resume_keywords + jd_keywords))

        # Generate queries (example strategies)
        queries = []
        # 1. Core role + top keywords
        if combined_keywords:
            query1 = f"{role_title} technical concepts related to {', '.join(combined_keywords[:4])}"
            queries.append(clean_text(query1))
        # 2. Specific keywords overlap
        overlap_keywords = list(set(resume_keywords) & set(jd_keywords))
        if overlap_keywords:
            query2 = f"Explain {overlap_keywords[0]} and {overlap_keywords[1] if len(overlap_keywords)>1 else ''} for a {role_title}"
            queries.append(clean_text(query2))
        # 3. General query from JD keywords
        if jd_keywords:
            query3 = f"Common interview questions about {jd_keywords[0]} for {role_title}"
            queries.append(clean_text(query3))

        # Ensure we have at least one query, fallback if keyword extraction failed
        if not queries:
            queries.append(clean_text(f"{role_title}: {jd_summary} Candidate skills: {resume_summary}")[:500])

        final_queries = list(set(q for q in queries if q))[:num_queries] # Deduplicate and limit
        logger.info(f"Generated {len(final_queries)} RAG search queries.")
        return final_queries

    except Exception as e:
        logger.error(f"Error generating RAG search queries: {e}", exc_info=True)
        # Fallback to simple query
        combined_text = f"Job Description: {jd_summary} Resume Summary: {resume_summary}"
        return [clean_text(combined_text)[:500]]

def retrieve_similar_documents(query, top_k=None, threshold=None):
    """Retrieves similar documents from the database using vector similarity."""
    if config.RETRIEVAL_TOP_K <= 0 or db_cursor is None or embedding_model is None:
        # logger.debug("RAG retrieval skipped (disabled, DB not connected, or model not loaded).")
        return []

    top_k = top_k if top_k is not None else config.RETRIEVAL_TOP_K
    threshold = threshold if threshold is not None else config.RETRIEVAL_SIMILARITY_THRESHOLD

    try:
        logger.debug(f"Generating embedding for query: '{query[:50]}...'")
        query_embedding = embedding_model.encode(query)

        # --- Perform vector similarity search ---
        # Ensure the table name and embedding column name match your DB schema
        # The query uses cosine similarity (1 - cosine_distance)
        # Requires the pgvector extension enabled in PostgreSQL: CREATE EXTENSION IF NOT EXISTS vector;
        # Assumes embedding column is of type VECTOR(dimensions)
        sql_query = """
            SELECT id, content, 1 - (embedding <=> %s) AS similarity
            FROM knowledge_documents
            WHERE 1 - (embedding <=> %s) >= %s
            ORDER BY similarity DESC
            LIMIT %s;
        """
        # Convert numpy array to list for psycopg2 compatibility
        embedding_list = query_embedding.tolist()
        db_cursor.execute(sql_query, (embedding_list, embedding_list, threshold, top_k))
        results = db_cursor.fetchall()

        documents = [{"id": row[0], "content": row[1], "score": row[2]} for row in results]
        logger.info(f"Retrieved {len(documents)} documents for query '{query[:30]}...' (Threshold: {threshold}, TopK: {top_k})")
        return documents

    except psycopg2.Error as db_err:
         # Handle potential database errors (e.g., table not found, pgvector not enabled)
         logger.error(f"Database error during RAG retrieval: {db_err}", exc_info=True)
         # Attempt to reconnect if connection lost? (Simple approach: just log error)
         # Or re-raise? For now, return empty list.
         # Consider trying to reconnect or resetting db_connection/db_cursor here.
         return []
    except AttributeError as e:
        # Handle case where embedding_model or db_cursor became None unexpectedly
        logger.error(f"RAG component (model or cursor) became unavailable during retrieval: {e}", exc_info=True)
        return []
    except Exception as e:
        logger.error(f"Unexpected error during RAG document retrieval: {e}", exc_info=True)
        return []

def format_rag_context(documents, max_length=None):
    """Formats retrieved documents into a single string for the LLM context."""
    if not documents:
        return "No relevant context found in the knowledge base."

    max_length = max_length if max_length is not None else config.MAX_CONTEXT_LENGTH
    context_str = "--- Relevant Context from Knowledge Base ---\n\n"
    current_length = len(context_str)

    for doc in documents:
        doc_content = f"**Source ID {doc.get('id', 'N/A')} (Similarity: {doc.get('score', 0):.2f}):**\n{doc.get('content', '')}\n\n"
        if max_length is None or (current_length + len(doc_content) <= max_length):
            context_str += doc_content
            current_length += len(doc_content)
        else:
            # Stop adding documents if max length is reached
            logger.warning(f"RAG context truncated at {max_length} characters.")
            break

    return context_str.strip()

# --- Skill/Topic Extraction ---
def get_focus_topics(resume_text, jd_text, top_n=5):
    """Identifies key overlapping topics/skills between resume and JD."""
    if not NLTK_AVAILABLE or not nltk_initialized:
        logger.warning("NLTK unavailable. Cannot determine focus topics accurately.")
        return ["General skills based on JD"] # Fallback

    if not resume_text or not jd_text:
        return ["Review Resume/JD"]

    try:
        logger.debug("Extracting focus topics from resume and JD...")
        # Use keyword extraction (can be improved with phrase extraction or NER)
        resume_keywords = set(extract_keywords(resume_text, max_keywords=20))
        jd_keywords = set(extract_keywords(jd_text, max_keywords=20))

        # Find intersection (overlapping keywords)
        overlap = list(resume_keywords.intersection(jd_keywords))

        # If overlap is small, supplement with top JD keywords
        if len(overlap) < top_n:
             additional_topics = [kw for kw in jd_keywords if kw not in overlap]
             needed = top_n - len(overlap)
             overlap.extend(additional_topics[:needed])

        focus_topics = overlap[:top_n] if overlap else list(jd_keywords)[:top_n] # Fallback to JD keywords

        if not focus_topics: # Final fallback
             return ["General technical skills"]

        logger.info(f"Identified focus topics: {focus_topics}")
        return focus_topics

    except Exception as e:
        logger.error(f"Error extracting focus topics: {e}", exc_info=True)
        return ["General technical skills"]


# --- Project Details Extraction ---
def extract_project_details(resume_text, max_length=None):
    """
    Extracts sections likely related to Projects or Experience from resume text.
    Uses simple heuristics based on common section headers.
    """
    if not resume_text: return ""
    max_length = max_length if max_length is not None else config.MAX_PROJECT_SUMMARY_LENGTH

    # Common headers for projects/experience sections
    project_headers = [
        "projects", "personal projects", "academic projects",
        "experience", "work experience", "professional experience",
        "relevant experience"
    ]
    # Headers that usually mark the *end* of relevant sections
    end_headers = [
        "skills", "technical skills", "languages", "tools", "technologies",
        "education", "certifications", "awards", "publications", "references",
        "interests", "hobbies"
    ]

    lines = resume_text.split('\n')
    extracted_details = ""
    in_project_section = False
    current_length = 0

    for line in lines:
        line_strip = line.strip()
        if not line_strip: continue

        line_lower = line_strip.lower()

        # Check for end headers first to stop extraction
        if any(line_lower.startswith(header) for header in end_headers):
             # More specific check: ensure it looks like a header (e.g., short line, potential colon/underline)
             if len(line_strip) < 30 and re.match(r'^[a-zA-Z\s]+[:\-—_]?$', line_strip):
                  in_project_section = False
                  logger.debug(f"Detected end header: '{line_strip}', stopping project extraction.")
                  continue # Stop processing lines if we hit skills/education etc.

        # Check for project/experience headers
        if any(line_lower.startswith(header) for header in project_headers):
             if len(line_strip) < 30 and re.match(r'^[a-zA-Z\s/]+[:\-—_]?$', line_strip):
                 in_project_section = True
                 logger.debug(f"Entering project/experience section: '{line_strip}'")
                 # Optionally include the header itself? Maybe not needed.
                 # extracted_details += line_strip + "\n"
                 # current_length += len(line_strip) + 1
                 continue # Move to the next line after finding header

        # If currently inside a project/experience section, add the line
        if in_project_section:
            line_to_add = line_strip + "\n"
            if max_length is None or (current_length + len(line_to_add) <= max_length):
                 extracted_details += line_to_add
                 current_length += len(line_to_add)
            else:
                 logger.debug("Project details reached max length, stopping extraction for this section.")
                 in_project_section = False # Stop adding from this section if limit hit
                 # Optionally add truncation indicator:
                 # extracted_details += "... (truncated)\n"
                 # break # Stop entirely if needed

    if not extracted_details:
         logger.warning("Could not definitively identify Project/Experience sections in resume using headers.")
         # Fallback: return a portion of the middle? Or just empty? Let's return empty for now.
         return ""

    logger.info(f"Extracted {len(extracted_details)} characters of potential project/experience details.")
    return extracted_details.strip()


# --- Cleanup function (Optional) ---
def close_resources():
    """Closes database connection if open."""
    global db_connection, db_cursor
    if db_cursor:
        try:
            db_cursor.close()
            logger.info("Database cursor closed.")
        except Exception as e:
            logger.error(f"Error closing database cursor: {e}", exc_info=True)
        db_cursor = None
    if db_connection:
        try:
            db_connection.close()
            logger.info("Database connection closed.")
        except Exception as e:
            logger.error(f"Error closing database connection: {e}", exc_info=True)
        db_connection = None

/* General Body Styling (Example) */
body {
    font-family: sans-serif;
    line-height: 1.6;
    margin: 0;
    padding: 0;
    background-color: #f4f4f4;
    color: #333;
}

header, footer {
    background-color: #333;
    color: #fff;
    padding: 1rem 2rem;
    text-align: center;
}

main {
    max-width: 1400px; /* Adjust max width as needed */
    margin: 20px auto; /* Center the main content */
    padding: 20px;
    background-color: #fff;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
}

h1, h2, h3 {
    color: #333;
}

/* Core Layout Styling */
.interview-container {
    display: flex;
    flex-wrap: wrap; /* Allow wrapping on smaller screens */
    gap: 30px; /* Space between avatar and interaction sections */
}

.avatar-section {
    flex: 1; /* Allows it to grow/shrink */
    min-width: 400px; /* Minimum width before wrapping */
    /* You might want a max-width too */
    /* max-width: 700px; */
}

.interaction-section {
    flex: 1; /* Allows it to grow/shrink */
    min-width: 400px; /* Minimum width */
}

/* Styling for the Avatar Stream Wrapper and Iframe */
.avatar-stream-wrapper {
    width: 100%;
    /* Maintain aspect ratio (e.g., 16:9) */
    /* padding-top: 56.25%; /* 9 / 16 * 100% */
    /* position: relative; */
    /* Alternatively, set fixed height: */
    height: 480px; /* Example fixed height */
    background-color: #000; /* Black background while loading */
    border: 1px solid #ccc;
    overflow: hidden; /* Hide anything outside the border */
}

.avatar-stream-iframe {
    /* If using aspect ratio padding-top: */
    /* position: absolute;
    top: 0;
    left: 0; */
    width: 100%;
    height: 100%;
    border: none; /* Redundant with wrapper border */
}

/* Styling for Interaction Section Elements */
#startForm div {
    margin-bottom: 15px;
}

label {
    display: block;
    margin-bottom: 5px;
    font-weight: bold;
}

input[type="file"],
textarea {
    width: 95%; /* Adjust width */
    padding: 8px;
    border: 1px solid #ccc;
    border-radius: 4px;
}

textarea {
    resize: vertical; /* Allow vertical resizing */
}

button {
    padding: 10px 15px;
    background-color: #5cb85c;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    font-size: 1em;
    margin-right: 10px;
    margin-top: 10px;
}

button:hover {
    background-color: #4cae4c;
}

button:disabled {
    background-color: #ccc;
    cursor: not-allowed;
}

#controls {
    margin-top: 20px;
    padding-top: 20px;
    border-top: 1px solid #eee;
}

.message-display {
    min-height: 100px;
    border: 1px solid #e0e0e0;
    background-color: #f8f8f8;
    padding: 15px;
    margin-bottom: 20px;
    white-space: pre-wrap; /* Respect newlines from AI */
    font-size: 1.1em;
    border-radius: 4px;
}

#status {
    margin-top: 15px;
    font-style: italic;
    color: #666;
}

.error-message {
    color: #d9534f; /* Red color for errors */
    margin-top: 10px;
    font-weight: bold;
}

#audioPlayback {
    width: 100%;
    margin-top: 15px;
}


/* Responsive adjustments (Example) */
@media (max-width: 800px) {
    .interview-container {
        flex-direction: column; /* Stack elements vertically */
    }
    .avatar-section, .interaction-section {
        min-width: 100%; /* Take full width when stacked */
    }
    .avatar-stream-wrapper {
        height: 300px; /* Adjust height for smaller screens */
    }
}

// --- Global Variables (Keep existing ones like mediaRecorder, audioChunks, etc.) ---
let mediaRecorder;
let audioChunks = [];
let interviewId = null;
let audioBlob = null; // To store the final Blob

// --- DOM Element References (Update these!) ---
// Get references to the elements using their IDs from the updated HTML
const startForm = document.getElementById('startForm'); // This ID should exist in the HTML
const interviewControlsDiv = document.getElementById('interviewControls'); // The div holding controls/text
const aiMessageTextElement = document.getElementById('aiMessageText');
const recordButton = document.getElementById('recordButton');
const stopButton = document.getElementById('stopButton');
const getReportButton = document.getElementById('getReportButton');
const statusElement = document.getElementById('status');
const errorDisplayElement = document.getElementById('errorDisplay');
const audioPlaybackElement = document.getElementById('audioPlayback');
// Input fields (if needed directly)
const resumeInput = document.getElementById('resume');
const jdInput = document.getElementById('job_description');

// --- Utility Functions (Keep existing ones like displayError, updateStatus) ---
function displayError(message) {
    if (errorDisplayElement) {
        errorDisplayElement.textContent = message;
        errorDisplayElement.style.display = 'block'; // Show error area
    } else {
        console.error("Error display element not found!");
    }
    console.error("Error:", message); // Also log to console
}

function clearError() {
    if (errorDisplayElement) {
        errorDisplayElement.textContent = '';
        errorDisplayElement.style.display = 'none'; // Hide error area
    }
}

function updateStatus(message) {
    if (statusElement) {
        statusElement.textContent = `Status: ${message}`;
    } else {
        console.warn("Status element not found!");
    }
}

// --- Core Interview Logic Functions ---

// Function to handle the submission of the initial setup form
async function handleStartInterview(event) {
    event.preventDefault(); // Prevent default form submission (page reload)
    clearError();
    updateStatus("Initializing interview...");

    // Disable form temporarily
    const submitButton = startForm ? startForm.querySelector('button[type="submit"]') : null;
    if (submitButton) submitButton.disabled = true;


    // Validate inputs (basic)
    if (!resumeInput || !resumeInput.files || resumeInput.files.length === 0) {
        displayError("Please select a resume file (PDF).");
        if (submitButton) submitButton.disabled = false;
        updateStatus("Initialization failed.");
        return;
    }
     if (!jdInput || !jdInput.value.trim()) {
        displayError("Please paste the job description.");
        if (submitButton) submitButton.disabled = false;
        updateStatus("Initialization failed.");
        return;
    }

    const formData = new FormData(startForm); // Get form data

    try {
        const response = await fetch('/start-interview', {
            method: 'POST',
            body: formData, // Send form data including the file
        });

        const data = await response.json();

        if (!response.ok) {
             // Handle specific errors like 400, 413, 500
            throw new Error(data.error || `HTTP error! status: ${response.status}`);
        }

        // --- SUCCESS ---
        interviewId = data.interview_id;
        updateStatus("Interview initialized. Waiting for greeting...");
        console.log("Interview started with ID:", interviewId);

        // Hide the setup form and show the interview controls
        if (startForm) startForm.style.display = 'none';
        if (interviewControlsDiv) interviewControlsDiv.style.display = 'block';

        // Automatically fetch the first AI message (greeting)
        fetchAiMessage();

    } catch (error) {
        displayError(`Initialization failed: ${error.message}`);
        updateStatus("Initialization failed.");
        // Re-enable form on error
         if (submitButton) submitButton.disabled = false;
    }
}

// Function to fetch the next AI message
async function fetchAiMessage() {
    if (!interviewId) {
        displayError("No active interview ID.");
        return;
    }
    updateStatus("Getting AI message...");
    clearError(); // Clear previous errors

    try {
        const response = await fetch('/get-ai-message');
        const data = await response.json();

        if (!response.ok || data.error) {
             throw new Error(data.error || `HTTP error! status: ${response.status}`);
        }

        // Display AI message
        if (aiMessageTextElement) {
            aiMessageTextElement.textContent = `AI: ${data.ai_message}`;
        } else {
            console.error("AI message text element not found!");
        }

        // Update UI based on interview state
        updateInterviewUI(data.status); // Pass the state received from backend

    } catch (error) {
        displayError(`Error fetching AI message: ${error.message}`);
        updateStatus("Error getting message.");
        // Decide how to handle UI on error (e.g., disable recording?)
        updateInterviewUI('ERROR'); // Assume error state for UI
    }
}

// --- Audio Recording Functions (Keep existing startRecording, stopRecording) ---
async function startRecording() {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        displayError("Microphone access (getUserMedia) not supported by your browser.");
        return;
    }
    clearError();
    audioChunks = []; // Reset chunks

    try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        // Determine mimeType - Check browser support
        const options = { mimeType: 'audio/webm;codecs=opus' }; // Prefer webm/opus
         if (!MediaRecorder.isTypeSupported(options.mimeType)) {
            console.warn(`${options.mimeType} not supported, trying audio/ogg...`);
            options.mimeType = 'audio/ogg;codecs=opus';
            if (!MediaRecorder.isTypeSupported(options.mimeType)) {
                 console.warn(`${options.mimeType} not supported, trying default...`);
                 delete options.mimeType; // Use browser default
            }
        }

        mediaRecorder = new MediaRecorder(stream, options);

        mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                audioChunks.push(event.data);
            }
        };

        mediaRecorder.onstop = async () => {
            audioBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType || 'audio/webm' }); // Use recorded mimeType or default
             console.log("Recording stopped. Blob size:", audioBlob.size, "Type:", audioBlob.type);
            // Optional: Provide playback
            if (audioPlaybackElement) {
                const audioUrl = URL.createObjectURL(audioBlob);
                audioPlaybackElement.src = audioUrl;
                audioPlaybackElement.style.display = 'block';
            }
            // Automatically submit after stopping
            await submitAudioResponse();

            // Stop microphone tracks
            stream.getTracks().forEach(track => track.stop());
        };

        mediaRecorder.onerror = (event) => {
            displayError(`Recorder Error: ${event.error.name} - ${event.error.message}`);
            updateStatus("Recording error.");
            // Update UI state if needed
            if (recordButton) recordButton.disabled = true;
            if (stopButton) stopButton.disabled = true;
        };

        mediaRecorder.start();
        updateStatus("Recording...");
        if (recordButton) recordButton.disabled = true;
        if (stopButton) stopButton.disabled = false; // Enable stop button

    } catch (err) {
        displayError(`Microphone Error: ${err.name} - ${err.message}. Please ensure microphone access is granted.`);
        updateStatus("Mic error.");
    }
}

function stopRecording() {
    if (mediaRecorder && mediaRecorder.state === "recording") {
        mediaRecorder.stop(); // This will trigger the 'onstop' event
        updateStatus("Processing recording...");
        if (stopButton) stopButton.disabled = true; // Disable stop button immediately
        // Record button remains disabled until next AI turn makes state AWAITING_RESPONSE
    } else {
        console.warn("Stop recording called but recorder not active.");
    }
}


// Function to submit the recorded audio
async function submitAudioResponse() {
    if (!audioBlob || audioBlob.size === 0) {
        displayError("No audio recorded or recording is empty.");
        updateStatus("Submission failed.");
        // Consider re-enabling record button if appropriate for the flow
        // updateInterviewUI('AWAITING_RESPONSE'); // Or based on actual state
        return;
    }
    if (!interviewId) {
         displayError("No active interview ID for submission.");
         updateStatus("Submission failed.");
         return;
    }

    updateStatus("Submitting response...");
    clearError();

    const formData = new FormData();
    // Use a filename that the backend might expect or find useful
    const filename = `interview_${interviewId}_response.webm`; // Example filename
    formData.append('audio_data', audioBlob, filename);

    try {
        const response = await fetch('/submit-response', {
            method: 'POST',
            body: formData,
        });

        const data = await response.json();

        if (!response.ok) {
            throw new Error(data.error || `HTTP error! status: ${response.status}`);
        }

        // --- SUCCESS ---
        updateStatus("Response submitted. Waiting for next AI message...");
        console.log("Audio submitted successfully.");
        if (audioPlaybackElement) audioPlaybackElement.style.display = 'none'; // Hide player after successful submit
        audioBlob = null; // Clear blob after submission

        // Fetch the next AI message automatically
        fetchAiMessage();

    } catch (error) {
        displayError(`Submission failed: ${error.message}`);
        updateStatus("Submission failed.");
        // Decide how to handle UI - maybe allow recording again?
        // This depends on whether backend state changed despite error
        // Best might be to fetch status again or rely on next fetchAiMessage
         updateInterviewUI('AWAITING_RESPONSE'); // Tentatively allow retry
    }
}

// --- Report Function (Keep existing getReport) ---
async function getReport() {
    if (!interviewId) {
        displayError("No active interview ID to get report for.");
        return;
    }
    updateStatus("Generating report...");
    clearError();
    if (getReportButton) getReportButton.disabled = true; // Disable while fetching

    try {
        const response = await fetch('/get-report');

        if (!response.ok) {
             // Try to get error message from JSON response if available
             let errorMsg = `Report generation failed with status: ${response.status}`;
             try {
                const data = await response.json();
                errorMsg = data.error || errorMsg;
             } catch(jsonError) {
                 // Ignore if response wasn't JSON
             }
            throw new Error(errorMsg);
        }

        // Handle PDF download
        const blob = await response.blob();
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        // Extract filename from Content-Disposition header if possible, otherwise use default
        const disposition = response.headers.get('Content-Disposition');
        let filename = `interview_report_${interviewId}.pdf`; // Default
        if (disposition && disposition.indexOf('attachment') !== -1) {
            const filenameRegex = /filename[^;=\n]*=((['"]).*?\2|[^;\n]*)/;
            const matches = filenameRegex.exec(disposition);
            if (matches != null && matches[1]) {
            filename = matches[1].replace(/['"]/g, '');
            }
        }
        a.download = filename;
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);

        updateStatus("Report downloaded.");


    } catch (error) {
        displayError(`Failed to get report: ${error.message}`);
        updateStatus("Report generation failed.");
    } finally {
         // Re-enable button only if state allows (e.g., FINISHED)
         // We need to know the state again. Let's assume it's still FINISHED
         // A better approach might be to fetch state again after report attempt.
         updateInterviewUI('FINISHED');
    }
}


// --- UI State Management ---
function updateInterviewUI(state) {
     console.log("Updating UI for state:", state); // Log current state
     updateStatus(state); // Update the status text

    // Ensure elements exist before trying to set properties
    const controlsExist = recordButton && stopButton && getReportButton;
    if (!controlsExist) {
        console.warn("One or more control buttons not found. Cannot update UI.");
        return;
    }

    switch (state) {
        case 'INITIALIZING':
        case 'READY': // After initialization, before greeting
            recordButton.disabled = true;
            stopButton.disabled = true;
            getReportButton.disabled = true;
            if (startForm) startForm.style.display = 'none'; // Hide form
             if (interviewControlsDiv) interviewControlsDiv.style.display = 'block'; // Show controls
            break;
        case 'AWAITING_RESPONSE': // AI has asked a question, waiting for user
            recordButton.disabled = false; // Enable recording
            stopButton.disabled = true;
            getReportButton.disabled = true;
            if (startForm) startForm.style.display = 'none';
             if (interviewControlsDiv) interviewControlsDiv.style.display = 'block';
            break;
        case 'IN_PROGRESS': // User has recorded, backend is processing
        case 'ASKING': // Backend is generating AI response
        case 'EVALUATING': // Backend is evaluating after finish
            recordButton.disabled = true;
            stopButton.disabled = true;
            getReportButton.disabled = true;
             if (startForm) startForm.style.display = 'none';
             if (interviewControlsDiv) interviewControlsDiv.style.display = 'block';
            break;
         case 'RECORDING': // Custom state used only by frontend during recording
             recordButton.disabled = true;
             stopButton.disabled = false; // Stop should be enabled
             getReportButton.disabled = true;
             if (startForm) startForm.style.display = 'none';
             if (interviewControlsDiv) interviewControlsDiv.style.display = 'block';
            break;
        case 'FINISHED': // Interview complete, evaluation may or may not be done
            recordButton.disabled = true;
            stopButton.disabled = true;
            getReportButton.disabled = false; // Enable report download
            if (startForm) startForm.style.display = 'none';
             if (interviewControlsDiv) interviewControlsDiv.style.display = 'block';
            break;
        case 'ERROR':
             // Decide how to handle errors - often disable most actions
             recordButton.disabled = true;
             stopButton.disabled = true;
             getReportButton.disabled = true; // Or maybe allow report if finished before error?
             if (startForm) startForm.style.display = 'block'; // Show form again? Or keep controls visible?
             if (interviewControlsDiv) interviewControlsDiv.style.display = 'block'; // Keep controls visible to show error context
            break;
        default: // Initial page load state before interaction
            recordButton.disabled = true;
            stopButton.disabled = true;
            getReportButton.disabled = true;
             if (startForm) startForm.style.display = 'block'; // Show form initially
            if (interviewControlsDiv) interviewControlsDiv.style.display = 'none'; // Hide controls initially
            updateStatus("Idle"); // Set initial status text
    }
}


// --- Event Listeners ---
document.addEventListener('DOMContentLoaded', () => {
    console.log("DOM fully loaded and parsed");

    // Check if elements exist right away
    if (!startForm) {
        console.error("Setup form ('startForm') not found on DOM load!");
    }
     if (!interviewControlsDiv) {
        console.error("Interview controls container ('interviewControls') not found on DOM load!");
     }
    if (!aiMessageTextElement || !recordButton || !stopButton || !getReportButton || !statusElement || !errorDisplayElement || !audioPlaybackElement) {
         console.error("One or more essential interview control elements are missing!");
    }

    // Attach event listener to the form
    if (startForm) {
        startForm.addEventListener('submit', handleStartInterview);
        console.log("Event listener attached to startForm.");
    } else {
        // Logged error above is sufficient
    }

    // Attach listeners to buttons (ensure they exist first)
    if (recordButton) {
        recordButton.addEventListener('click', startRecording);
    }
    if (stopButton) {
        stopButton.addEventListener('click', stopRecording);
    }
    if (getReportButton) {
        getReportButton.addEventListener('click', getReport);
    }

    // Set initial UI state
    updateInterviewUI('INITIAL'); // Or whatever your very first state is called

}); // End DOMContentLoaded

console.log("main.js script loaded"); // Log script load

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Interview Simulation</title>
    <!-- Link to your CSS file -->
    <link
      rel="stylesheet"
      href="{{ url_for('static', filename='css/style.css') }}"
    />
  </head>
  <body>
    <header>
      <h1>AI Interview Simulation</h1>
      <!-- Optional: Add logo or other header elements -->
    </header>

    <main class="interview-container">
      <!-- Section for the Pixel Streaming Avatar -->
      <section class="avatar-section">
        <h2>Interviewer Avatar</h2>
        <div class="avatar-stream-wrapper">
          <iframe
            id="pixelStreamerFrame"
            src="http://127.0.0.1/"
            class="avatar-stream-iframe"
            frameborder="0"
            allow="autoplay"
            title="Pixel Streaming Avatar"
          >
            <!-- Fallback content -->
            Pixel Stream unavailable or not supported by your browser. Ensure
            Unreal Engine stream is running at http://127.0.0.1/
          </iframe>
        </div>
      </section>

      <!-- Section for Interview Interaction -->
      <section class="interaction-section">
        <h2>Interview</h2>

        <!-- File Upload Form (Initially Visible) -->
        <form id="startForm">
          <h3>Setup Interview</h3>
          <div>
            <label for="resume">Upload Resume (PDF):</label>
            <input
              type="file"
              id="resume"
              name="resume"
              accept=".pdf"
              required
            />
          </div>
          <div>
            <label for="job_description">Paste Job Description:</label><br />
            <textarea
              id="job_description"
              name="job_description"
              rows="8"
              required
            ></textarea>
          </div>
          <button type="submit">Start Interview</button>
        </form>

        <!-- Interview Controls (Initially Hidden or Disabled) -->
        <div id="interviewControls" style="display: none">
          <h3>Conversation</h3>
          <div id="aiMessageText" class="message-display">
            Waiting for interview to start...
          </div>

          <div id="controls">
            <button id="recordButton" disabled>Start Recording Answer</button>
            <button id="stopButton" disabled>Stop Recording</button>
            <button id="getReportButton" disabled>Get Interview Report</button>
            <p id="status">Status: Idle</p>
            <div id="errorDisplay" class="error-message"></div>
            <audio
              id="audioPlayback"
              controls
              style="display: none; margin-top: 10px"
            ></audio>
          </div>
        </div>
      </section>
    </main>

    <footer>
      <!-- Optional footer content -->
    </footer>

    <!-- Link to your JavaScript file -->
    <script src="{{ url_for('static', filename='js/main.js') }}"></script>
  </body>
</html>

# # app.py
# print("--- app.py: Starting script execution ---") # DEBUG
# import os
# import uuid
# import logging
# from threading import Lock
# import traceback # For detailed error logging
# import time # Import time for start_interview elapsed time

# print("--- app.py: Basic imports done ---") # DEBUG

# from flask import Flask, request, jsonify, render_template, send_file, session as flask_session
# from werkzeug.utils import secure_filename
# import werkzeug.exceptions # Import explicitly for error handling

# print("--- app.py: Flask imports done ---") # DEBUG

# # Local Imports - Use relative imports if running as a package,
# # or ensure modules directory is in PYTHONPATH if running app.py directly
# try:
#     print("--- app.py: Attempting local imports ---") # DEBUG
#     import config
#     print("--- app.py: Imported config ---") # DEBUG
#     from modules import utils
#     print("--- app.py: Imported modules.utils ---") # DEBUG
#     from modules import llm_interface
#     print("--- app.py: Imported modules.llm_interface ---") # DEBUG
#     from modules import audio_utils
#     print("--- app.py: Imported modules.audio_utils ---") # DEBUG
#     from modules import report_generator
#     print("--- app.py: Imported modules.report_generator ---") # DEBUG
#     from modules.interview_logic import InterviewSession
#     print("--- app.py: Imported modules.interview_logic.InterviewSession ---") # DEBUG
#     print("--- app.py: Local imports successful ---") # DEBUG
# except ImportError as e:
#      print(f"--- app.py: FATAL ERROR importing local modules: {e}. Make sure modules are in the correct path. ---") # DEBUG
#      raise # Re-raise the error if imports fail critically

# # --- Basic Logging Setup ---
# print("--- app.py: Setting up logging ---") # DEBUG
# log_level = getattr(logging, config.LOG_LEVEL, logging.INFO)
# logging.basicConfig(level=log_level,
#                     format='%(asctime)s - %(name)s:%(lineno)d - %(levelname)s - %(message)s')
# # Reduce verbosity of libraries if needed
# logging.getLogger("werkzeug").setLevel(logging.WARNING)
# logging.getLogger("google").setLevel(logging.WARNING) # Gemini/STT libs
# logging.getLogger("urllib3").setLevel(logging.WARNING) # Often noisy via requests
# logging.getLogger("pdfminer").setLevel(logging.WARNING)
# logging.getLogger("PIL").setLevel(logging.WARNING) # pdfplumber dependency

# logger = logging.getLogger(__name__) # Logger for this application
# print(f"--- app.py: Logger '{__name__}' configured ---") # DEBUG

# # --- Flask App Initialization ---
# print("--- app.py: Initializing Flask app ---") # DEBUG
# app = Flask(__name__)
# print(f"--- app.py: Flask app object created: {app} ---") # DEBUG
# app.config['SECRET_KEY'] = config.SECRET_KEY
# app.config['UPLOAD_FOLDER'] = config.UPLOAD_FOLDER
# app.config['REPORT_FOLDER'] = config.REPORT_FOLDER
# app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024 # 32 MB
# print("--- app.py: Flask app configured ---") # DEBUG

# # Ensure upload and report directories exist
# try:
#     print("--- app.py: Creating upload/report directories (if needed) ---") # DEBUG
#     os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
#     os.makedirs(app.config['REPORT_FOLDER'], exist_ok=True)
#     logger.info(f"Upload folder: {app.config['UPLOAD_FOLDER']}")
#     logger.info(f"Report folder: {app.config['REPORT_FOLDER']}")
#     print("--- app.py: Directories ensured ---") # DEBUG
# except OSError as e:
#      logger.error(f"Error creating directories: {e}. Check permissions.")
#      print(f"--- app.py: FATAL ERROR creating directories: {e} ---") # DEBUG
#      # Depending on severity, might want to exit
#      import sys; sys.exit(1) # Exit if dirs can't be created

# # --- In-Memory Session Storage ---
# print("--- app.py: Setting up session storage ---") # DEBUG
# interview_sessions = {}
# session_lock = Lock()
# print("--- app.py: Session storage ready ---") # DEBUG

# # --- Initialize External Clients & Resources ---
# # We move initialization here BEFORE app.run to catch errors earlier
# # The @app.before_request might not run if app.run fails to start.
# initialization_complete = False
# try:
#     print("--- app.py: Performing pre-run initializations... ---") # DEBUG
#     logger.info("Performing pre-run initializations...")

#     # Initialize NLTK data
#     print("--- app.py: Initializing NLTK... ---") # DEBUG
#     utils.initialize_nltk()
#     print("--- app.py: NLTK initialized (or skipped). ---") # DEBUG

#     # Initialize STT Client
#     print("--- app.py: Initializing STT Client... ---") # DEBUG
#     if not audio_utils.STT_CLIENT:
#          stt_initialized = audio_utils.initialize_stt_client()
#          if not stt_initialized:
#               # Log warning but don't necessarily stop server? Depends on if STT is critical path for *any* function.
#               logger.warning("STT Client failed to initialize during pre-run. Transcription will be unavailable.")
#          else:
#               logger.info("STT Client initialized successfully during pre-run.")
#     else:
#           logger.info("STT Client already initialized.")
#     print("--- app.py: STT Client initialization attempted. ---") # DEBUG

#     # Initialize RAG/Embedding Model
#     print("--- app.py: Initializing RAG... ---") # DEBUG
#     utils.initialize_rag() # This handles checks for enablement/dependencies internally
#     logger.info("RAG system initialization attempted during pre-run.")
#     print("--- app.py: RAG initialization attempted. ---") # DEBUG

#     # Initialize LLMs? Optional, they lazy load anyway.
#     # print("--- app.py: Initializing LLMs... ---") # DEBUG
#     # llm_interface.initialize_llms()
#     # print("--- app.py: LLMs initialization attempted. ---") # DEBUG

#     initialization_complete = True
#     print("--- app.py: Pre-run initializations complete. ---") # DEBUG

# except Exception as init_err:
#      logger.exception(f"FATAL ERROR during pre-run initialization: {init_err}")
#      print(f"--- app.py: FATAL ERROR during pre-run initialization: {init_err} ---") # DEBUG
#      # Exit if critical initializations fail
#      import sys; sys.exit(1)


# # --- Utility Functions ---
# def allowed_file(filename, allowed_extensions):
#     return '.' in filename and \
#            filename.rsplit('.', 1)[1].lower() in allowed_extensions

# def get_session(interview_id) -> InterviewSession | None:
#      with session_lock:
#           session = interview_sessions.get(interview_id)
#           # Reduced logging noise here
#           # if not session: logger.warning(f"Session {interview_id} not found in memory.")
#           return session

# def store_session(interview_id, session_obj):
#      with session_lock:
#           interview_sessions[interview_id] = session_obj
#           logger.info(f"Stored session {interview_id}. Active sessions: {len(interview_sessions)}")

# def remove_session(interview_id):
#      with session_lock:
#           if interview_id in interview_sessions:
#                del interview_sessions[interview_id]
#                logger.info(f"Removed session {interview_id}. Active sessions: {len(interview_sessions)}")
#           # else: logger.warning(f"Attempted to remove non-existent session {interview_id}.")
# print("--- app.py: Utility functions defined ---") # DEBUG

# # --- Routes ---
# @app.route('/')
# def index():
#     print("--- app.py: Route / accessed ---") # DEBUG
#     return render_template('index.html')

# @app.route('/start-interview', methods=['POST'])
# def start_interview():
#     start_time = time.time()
#     # ... (rest of the route function - keep as is) ...
#     # No changes needed inside the route for this debug step
#     logger.info("Received request to start new interview.")
#     try:
#         if 'resume' not in request.files:
#             logger.warning("Resume file part missing in request.")
#             return jsonify({"error": "Resume file is required."}), 400
#         resume_file = request.files['resume']
#         jd_text = request.form.get('job_description', '').strip()

#         if not resume_file or resume_file.filename == '':
#             logger.warning("No resume file selected.")
#             return jsonify({"error": "No resume file selected."}), 400
#         if not jd_text:
#             logger.warning("Job description text is empty.")
#             return jsonify({"error": "Job description cannot be empty."}), 400

#         if not allowed_file(resume_file.filename, {'pdf'}):
#             logger.warning(f"Invalid file type uploaded: {resume_file.filename}")
#             return jsonify({"error": "Invalid file type. Please upload a PDF resume."}), 400

#         temp_filename = f"{uuid.uuid4()}_{secure_filename(resume_file.filename)}"
#         temp_resume_path = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)

#         try:
#             logger.debug(f"Saving temporary resume to: {temp_resume_path}")
#             resume_file.save(temp_resume_path)
#             logger.debug("Extracting text from resume PDF...")
#             resume_text = utils.extract_text_from_pdf(temp_resume_path)
#             if resume_text is None or not resume_text.strip():
#                  logger.error(f"Failed to extract text from resume: {temp_resume_path}")
#                  if os.path.exists(temp_resume_path): os.remove(temp_resume_path)
#                  return jsonify({"error": "Failed to extract text from the uploaded resume PDF. It might be image-based or corrupted."}), 400
#             logger.info(f"Resume text extracted (length: {len(resume_text)} chars).")

#         except Exception as pdf_err:
#             logger.error(f"Error processing resume PDF: {pdf_err}", exc_info=True)
#             if os.path.exists(temp_resume_path): os.remove(temp_resume_path)
#             error_msg = f"Failed to process resume PDF: {pdf_err}"
#             if isinstance(pdf_err, ImportError):
#                  error_msg = "PDF processing library (pdfplumber) not installed or accessible."
#             return jsonify({"error": error_msg}), 500
#         finally:
#             if os.path.exists(temp_resume_path):
#                 try: os.remove(temp_resume_path)
#                 except OSError as e: logger.warning(f"Could not remove temp resume file {temp_resume_path}: {e}")

#         interview_id = str(uuid.uuid4())
#         logger.info(f"[{interview_id}] Generated new interview ID.")
#         old_interview_id = flask_session.pop('interview_id', None)
#         if old_interview_id: remove_session(old_interview_id)

#         logger.info(f"[{interview_id}] Creating InterviewSession object...")
#         session_obj = InterviewSession(interview_id, resume_text, jd_text)
#         init_state_info = session_obj.get_state()

#         if init_state_info["state"] == "ERROR":
#             logger.error(f"[{interview_id}] Session initialization failed. Error: {init_state_info['error']}")
#             return jsonify({"error": f"Interview initialization failed: {init_state_info['error']}"}), 500

#         store_session(interview_id, session_obj)
#         flask_session['interview_id'] = interview_id
#         logger.debug(f"Stored interview_id {interview_id} in Flask session.")
#         elapsed_time = time.time() - start_time
#         logger.info(f"[{interview_id}] Interview setup completed in {elapsed_time:.2f} seconds.")
#         return jsonify({"interview_id": interview_id,"message": "Interview initialized successfully. Ready for greeting."}), 200

#     except Exception as e:
#         logger.exception("Unexpected error in /start-interview")
#         if isinstance(e, werkzeug.exceptions.RequestEntityTooLarge):
#               return jsonify({"error": f"File upload failed. The resume file may be too large (limit: {app.config['MAX_CONTENT_LENGTH'] // 1024 // 1024} MB)."}), 413
#         return jsonify({"error": f"An unexpected server error occurred during setup: {e}"}), 500

# # ... (other routes: /get-ai-message, /submit-response, /get-report - keep as is) ...
# @app.route('/get-ai-message', methods=['GET'])
# def get_ai_message():
#     # ... route code ...
#     interview_id = flask_session.get('interview_id')
#     if not interview_id: return jsonify({"error": "No active interview session found. Please start a new interview."}), 400
#     logger.info(f"[{interview_id}] Received request for next AI message.")
#     session_obj = get_session(interview_id)
#     if not session_obj:
#         flask_session.pop('interview_id', None)
#         return jsonify({"error": "Interview session not found or expired. Please start again."}), 404
#     try:
#         state_info = session_obj.get_state()
#         current_state = state_info["state"]
#         logger.debug(f"[{interview_id}] Current session state: {current_state}")
#         ai_message = None
#         response_status = 200
#         if current_state == "READY": ai_message = session_obj.get_greeting()
#         elif current_state == "ASKING": ai_message = session_obj.get_next_ai_turn()
#         elif current_state == "AWAITING_RESPONSE": ai_message = session_obj.last_ai_message
#         elif current_state in ["FINISHED", "EVALUATING"]: ai_message = session_obj.last_ai_message or "The interview has concluded."
#         elif current_state == "ERROR":
#              ai_message = state_info["error"] or "An unspecified error occurred in the interview session."
#              response_status = 500
#         else: ai_message = "Interview is currently processing, please wait..."
#         final_state_info = session_obj.get_state()
#         if final_state_info["state"] == "ERROR":
#             ai_message = final_state_info["error"] or "An error occurred while generating the AI message."
#             response_status = 500
#         logger.info(f"[{interview_id}] Sending AI message. Final state: {final_state_info['state']}")
#         return jsonify({"ai_message": ai_message, "status": final_state_info["state"]}), response_status
#     except Exception as e:
#         logger.exception(f"[{interview_id}] Unexpected error in /get-ai-message")
#         if session_obj and session_obj.get_state()["state"] != "ERROR":
#              session_obj._set_error_state(f"Server error during AI turn generation: {e}")
#         return jsonify({"error": f"An unexpected server error occurred: {e}", "status": "ERROR"}), 500

# @app.route('/submit-response', methods=['POST'])
# def submit_response():
#     # ... route code ...
#     interview_id = flask_session.get('interview_id')
#     if not interview_id: return jsonify({"error": "No active interview session found. Please start a new interview."}), 400
#     logger.info(f"[{interview_id}] Received request to submit response.")
#     session_obj = get_session(interview_id)
#     if not session_obj:
#         flask_session.pop('interview_id', None)
#         return jsonify({"error": "Interview session not found or expired. Please start again."}), 404
#     current_state = session_obj.get_state()["state"]
#     if current_state != "AWAITING_RESPONSE":
#          return jsonify({"error": f"Cannot submit response now, the system is not awaiting your answer (state: {current_state})."}), 409
#     if 'audio_data' not in request.files: return jsonify({"error": "Audio data blob is required."}), 400
#     audio_file = request.files['audio_data']
#     if not audio_file or audio_file.filename == '': return jsonify({"error": "No audio file selected/uploaded."}), 400
#     file_ext = ".wav"
#     if '.' in audio_file.filename:
#         ext_candidate = audio_file.filename.rsplit('.', 1)[1].lower()
#         if ext_candidate in ['wav', 'webm', 'ogg', 'mp3', 'flac']: file_ext = f".{ext_candidate}"
#     qna_turn_number = session_obj.last_question_context.get('turn', session_obj.current_turn_number)
#     filename = secure_filename(f"{interview_id}_turn_{qna_turn_number}_response{file_ext}")
#     temp_audio_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
#     result = {"status": "error", "message": "Audio processing did not complete."}
#     response_status = 500 # Default error status
#     try:
#         logger.debug(f"[{interview_id}] Saving temporary audio response for QnA turn {qna_turn_number} to: {temp_audio_path}")
#         audio_file.save(temp_audio_path)
#         logger.info(f"[{interview_id}] Audio file saved successfully (Size: {os.path.getsize(temp_audio_path)} bytes).")
#         logger.info(f"[{interview_id}] Processing candidate response via InterviewSession...")
#         result = session_obj.process_candidate_response(temp_audio_path)
#         logger.info(f"[{interview_id}] Response processing result: {result}")
#         response_status = 200 if result.get("status") == "success" else 500
#     except werkzeug.exceptions.RequestEntityTooLarge:
#          logger.warning(f"[{interview_id}] Audio upload failed: File too large.")
#          result = {"status": "error", "message": f"Audio file is too large (limit: {app.config['MAX_CONTENT_LENGTH'] // 1024 // 1024} MB)."}
#          response_status = 413
#     except Exception as e:
#         logger.exception(f"[{interview_id}] Error saving or processing audio response")
#         result = {"status": "error", "message": f"Server error processing audio: {e}"}
#         if os.path.exists(temp_audio_path): os.remove(temp_audio_path)
#         response_status = 500
#     finally:
#         if os.path.exists(temp_audio_path):
#             try: os.remove(temp_audio_path)
#             except OSError as e: logger.warning(f"[{interview_id}] Could not remove temp audio file {temp_audio_path}: {e}")

#     if result.get("status") == "success":
#         return jsonify({"status": "OK", "message": "Response received and processed."}), response_status
#     else:
#         return jsonify({"error": result.get("message", "Failed to process response.")}), response_status


# @app.route('/get-report', methods=['GET'])
# def get_report():
#     # ... route code ...
#     interview_id = flask_session.get('interview_id')
#     if not interview_id: return jsonify({"error": "No active interview session found to generate report."}), 400
#     logger.info(f"[{interview_id}] Received request to get report.")
#     session_obj = get_session(interview_id)
#     if not session_obj:
#         flask_session.pop('interview_id', None)
#         return jsonify({"error": "Interview session not found or expired."}), 404
#     current_state = session_obj.get_state()["state"]
#     if current_state not in ["FINISHED", "EVALUATING", "ERROR"]:
#          return jsonify({"error": f"Cannot generate report yet. Interview status: {current_state}."}), 409
#     if not session_obj.evaluation_complete and current_state != "ERROR":
#          logger.info(f"[{interview_id}] Evaluation not complete. Performing final evaluation before report generation.")
#          eval_success = session_obj.perform_final_evaluation()
#          if not eval_success: logger.error(f"[{interview_id}] Final evaluation failed. Report may be incomplete.")
#          else: logger.info(f"[{interview_id}] Final evaluation completed.")
#     logger.info(f"[{interview_id}] Attempting to generate report...")
#     report_path = session_obj.generate_report()
#     if report_path and os.path.exists(report_path):
#         logger.info(f"[{interview_id}] Sending report file: {report_path}")
#         try:
#             return send_file(report_path, as_attachment=True, download_name=os.path.basename(report_path), mimetype='application/pdf')
#         except Exception as send_err:
#              logger.exception(f"[{interview_id}] Error sending report file {report_path}")
#              return jsonify({"error": f"Could not send report file: {send_err}"}), 500
#     else:
#         logger.error(f"[{interview_id}] Report generation failed or file not found. Report path: {report_path}")
#         error_msg = session_obj.get_state().get("error", "Failed to generate or find the interview report.")
#         return jsonify({"error": error_msg}), 500

# print("--- app.py: Routes defined ---") # DEBUG

# # --- Error Handling ---
# @app.errorhandler(404)
# def not_found_error(error):
#      logger.warning(f"404 Not Found: {request.url}")
#      return jsonify({"error": "Not Found", "message": "The requested URL was not found on the server."}), 404

# @app.errorhandler(400)
# def bad_request_error(error):
#      error_desc = error.description if hasattr(error, 'description') else 'No description provided.'
#      logger.warning(f"400 Bad Request: {request.url} - {error_desc}")
#      return jsonify({"error": "Bad Request", "message": error_desc}), 400

# @app.errorhandler(413)
# def request_entity_too_large_error(error):
#      logger.warning(f"413 Request Entity Too Large: {request.url} - Content Length: {request.content_length}")
#      limit_mb = app.config['MAX_CONTENT_LENGTH'] // 1024 // 1024
#      return jsonify({"error": "Request Entity Too Large", "message": f"The uploaded file exceeds the maximum allowed size of {limit_mb} MB."}), 413

# # Keep the generic exception handler
# @app.errorhandler(Exception)
# def handle_exception(e):
#     error_id = uuid.uuid4()
#     tb_str = traceback.format_exc()
#     logger.error(f"Unhandled Exception (ID: {error_id}): {e}\n--- Traceback ---\n{tb_str}--- End Traceback ---")
#     # Always return generic error in non-debug mode
#     return jsonify({"error": "An unexpected server error occurred. Please check server logs.",
#                      "error_id": str(error_id)}), 500

# print("--- app.py: Error handlers defined ---") # DEBUG


#flask run --host=0.0.0.0 --port=5050

#./SignallingWebServer/platform_scripts/bash/start.sh
#http://localhost:5050



# app.py
print("--- app.py: Starting script execution ---") # DEBUG
import os
import uuid
import logging
from threading import Lock
import traceback # For detailed error logging
import time # Import time for start_interview elapsed time

print("--- app.py: Basic imports done ---") # DEBUG

from flask import Flask, request, jsonify, render_template, send_file, session as flask_session
from werkzeug.utils import secure_filename
import werkzeug.exceptions # Import explicitly for error handling
from flask_cors import CORS # <-------------------- ADD THIS IMPORT

print("--- app.py: Flask imports done ---") # DEBUG

# Local Imports - Use relative imports if running as a package,
# or ensure modules directory is in PYTHONPATH if running app.py directly
try:
    print("--- app.py: Attempting local imports ---") # DEBUG
    import config
    print("--- app.py: Imported config ---") # DEBUG
    from modules import utils
    print("--- app.py: Imported modules.utils ---") # DEBUG
    from modules import llm_interface
    print("--- app.py: Imported modules.llm_interface ---") # DEBUG
    from modules import audio_utils
    print("--- app.py: Imported modules.audio_utils ---") # DEBUG
    from modules import report_generator
    print("--- app.py: Imported modules.report_generator ---") # DEBUG
    from modules.interview_logic import InterviewSession
    print("--- app.py: Imported modules.interview_logic.InterviewSession ---") # DEBUG
    print("--- app.py: Local imports successful ---") # DEBUG
except ImportError as e:
     print(f"--- app.py: FATAL ERROR importing local modules: {e}. Make sure modules are in the correct path. ---") # DEBUG
     raise # Re-raise the error if imports fail critically

# --- Basic Logging Setup ---
print("--- app.py: Setting up logging ---") # DEBUG
log_level = getattr(logging, config.LOG_LEVEL, logging.INFO)
logging.basicConfig(level=log_level,
                    format='%(asctime)s - %(name)s:%(lineno)d - %(levelname)s - %(message)s')
# Reduce verbosity of libraries if needed
logging.getLogger("werkzeug").setLevel(logging.WARNING)
logging.getLogger("google").setLevel(logging.WARNING) # Gemini/STT libs
logging.getLogger("urllib3").setLevel(logging.WARNING) # Often noisy via requests
logging.getLogger("pdfminer").setLevel(logging.WARNING)
logging.getLogger("PIL").setLevel(logging.WARNING) # pdfplumber dependency

logger = logging.getLogger(__name__) # Logger for this application
print(f"--- app.py: Logger '{__name__}' configured ---") # DEBUG

# --- Flask App Initialization ---
print("--- app.py: Initializing Flask app ---") # DEBUG
app = Flask(__name__)
print(f"--- app.py: Flask app object created: {app} ---") # DEBUG

# --- CORS Configuration --- # <-------------------- ADD THIS SECTION
# Allows requests from your React frontend development server (localhost:8080)
# supports_credentials=True is often needed for session cookies
# Use specific origins instead of "*" for better security
CORS(app, supports_credentials=True, resources={
    r"/*": {
        "origins": ["http://localhost:8080", "http://127.0.0.1:8080"] # Add other origins if needed
    }
})
print("--- app.py: CORS configured for http://localhost:8080 ---") # DEBUG
# --- End CORS Configuration --- #

app.config['SECRET_KEY'] = config.SECRET_KEY
app.config['UPLOAD_FOLDER'] = config.UPLOAD_FOLDER
app.config['REPORT_FOLDER'] = config.REPORT_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024 # 32 MB
print("--- app.py: Flask app configured ---") # DEBUG

# Ensure upload and report directories exist
try:
    print("--- app.py: Creating upload/report directories (if needed) ---") # DEBUG
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    os.makedirs(app.config['REPORT_FOLDER'], exist_ok=True)
    logger.info(f"Upload folder: {app.config['UPLOAD_FOLDER']}")
    logger.info(f"Report folder: {app.config['REPORT_FOLDER']}")
    print("--- app.py: Directories ensured ---") # DEBUG
except OSError as e:
     logger.error(f"Error creating directories: {e}. Check permissions.")
     print(f"--- app.py: FATAL ERROR creating directories: {e} ---") # DEBUG
     # Depending on severity, might want to exit
     import sys; sys.exit(1) # Exit if dirs can't be created

# --- In-Memory Session Storage ---
print("--- app.py: Setting up session storage ---") # DEBUG
interview_sessions = {}
session_lock = Lock()
print("--- app.py: Session storage ready ---") # DEBUG

# --- Initialize External Clients & Resources ---
# We move initialization here BEFORE app.run to catch errors earlier
# The @app.before_request might not run if app.run fails to start.
initialization_complete = False
try:
    print("--- app.py: Performing pre-run initializations... ---") # DEBUG
    logger.info("Performing pre-run initializations...")

    # Initialize NLTK data
    print("--- app.py: Initializing NLTK... ---") # DEBUG
    utils.initialize_nltk() # Make sure this function downloads if needed
    print("--- app.py: NLTK initialized (or skipped). ---") # DEBUG

    # Initialize STT Client
    print("--- app.py: Initializing STT Client... ---") # DEBUG
    if not audio_utils.STT_CLIENT:
         stt_initialized = audio_utils.initialize_stt_client()
         if not stt_initialized:
              # Log warning but don't necessarily stop server? Depends on if STT is critical path for *any* function.
              logger.warning("STT Client failed to initialize during pre-run. Transcription will be unavailable.")
         else:
              logger.info("STT Client initialized successfully during pre-run.")
    else:
          logger.info("STT Client already initialized.")
    print("--- app.py: STT Client initialization attempted. ---") # DEBUG

    # Initialize RAG/Embedding Model
    print("--- app.py: Initializing RAG... ---") # DEBUG
    # Make sure this function checks DB connection and table existence
    utils.initialize_rag()
    logger.info("RAG system initialization attempted during pre-run.")
    print("--- app.py: RAG initialization attempted. ---") # DEBUG

    # Initialize LLMs? Optional, they lazy load anyway.
    # print("--- app.py: Initializing LLMs... ---") # DEBUG
    # llm_interface.initialize_llms()
    # print("--- app.py: LLMs initialization attempted. ---") # DEBUG

    initialization_complete = True
    print("--- app.py: Pre-run initializations complete. ---") # DEBUG

except Exception as init_err:
     logger.exception(f"FATAL ERROR during pre-run initialization: {init_err}")
     print(f"--- app.py: FATAL ERROR during pre-run initialization: {init_err} ---") # DEBUG
     # Exit if critical initializations fail (like DB connection or essential models)
     import sys; sys.exit(1)


# --- Utility Functions ---
def allowed_file(filename, allowed_extensions):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in allowed_extensions

def get_session(interview_id) -> InterviewSession | None:
     with session_lock:
          session = interview_sessions.get(interview_id)
          # Reduced logging noise here
          # if not session: logger.warning(f"Session {interview_id} not found in memory.")
          return session

def store_session(interview_id, session_obj):
     with session_lock:
          interview_sessions[interview_id] = session_obj
          logger.info(f"Stored session {interview_id}. Active sessions: {len(interview_sessions)}")

def remove_session(interview_id):
     with session_lock:
          if interview_id in interview_sessions:
               del interview_sessions[interview_id]
               logger.info(f"Removed session {interview_id}. Active sessions: {len(interview_sessions)}")
          # else: logger.warning(f"Attempted to remove non-existent session {interview_id}.")
print("--- app.py: Utility functions defined ---") # DEBUG

# --- Routes ---
@app.route('/')
def index():
    print("--- app.py: Route / accessed ---") # DEBUG
    # This usually serves an HTML file, not directly relevant to API CORS
    return render_template('index.html')

@app.route('/start-interview', methods=['POST'])
def start_interview():
    start_time = time.time()
    logger.info("Received request to start new interview.")
    # Check Origin header for debugging CORS issues if they persist
    origin = request.headers.get('Origin')
    logger.debug(f"Request Origin: {origin}")

    try:
        if 'resume' not in request.files:
            logger.warning("Resume file part missing in request.")
            return jsonify({"error": "Resume file is required."}), 400
        resume_file = request.files['resume']
        jd_text = request.form.get('job_description', '').strip()

        if not resume_file or resume_file.filename == '':
            logger.warning("No resume file selected.")
            return jsonify({"error": "No resume file selected."}), 400
        if not jd_text:
            logger.warning("Job description text is empty.")
            return jsonify({"error": "Job description cannot be empty."}), 400

        if not allowed_file(resume_file.filename, {'pdf'}):
            logger.warning(f"Invalid file type uploaded: {resume_file.filename}")
            return jsonify({"error": "Invalid file type. Please upload a PDF resume."}), 400

        temp_filename = f"{uuid.uuid4()}_{secure_filename(resume_file.filename)}"
        temp_resume_path = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)

        try:
            logger.debug(f"Saving temporary resume to: {temp_resume_path}")
            resume_file.save(temp_resume_path)
            logger.debug("Extracting text from resume PDF...")
            resume_text = utils.extract_text_from_pdf(temp_resume_path)
            if resume_text is None or not resume_text.strip():
                 logger.error(f"Failed to extract text from resume: {temp_resume_path}")
                 if os.path.exists(temp_resume_path): os.remove(temp_resume_path)
                 return jsonify({"error": "Failed to extract text from the uploaded resume PDF. It might be image-based or corrupted."}), 400
            logger.info(f"Resume text extracted (length: {len(resume_text)} chars).")

        except Exception as pdf_err:
            logger.error(f"Error processing resume PDF: {pdf_err}", exc_info=True)
            if os.path.exists(temp_resume_path): os.remove(temp_resume_path)
            error_msg = f"Failed to process resume PDF: {pdf_err}"
            if isinstance(pdf_err, ImportError):
                 error_msg = "PDF processing library (pdfplumber) not installed or accessible."
            return jsonify({"error": error_msg}), 500
        finally:
            # Ensure temp file is removed even if text extraction fails partially
            if os.path.exists(temp_resume_path):
                try: os.remove(temp_resume_path)
                except OSError as e: logger.warning(f"Could not remove temp resume file {temp_resume_path}: {e}")

        interview_id = str(uuid.uuid4())
        logger.info(f"[{interview_id}] Generated new interview ID.")

        # Clear previous session *from memory* if one existed in the Flask session cookie
        old_interview_id = flask_session.pop('interview_id', None)
        if old_interview_id: remove_session(old_interview_id)

        logger.info(f"[{interview_id}] Creating InterviewSession object...")
        # Ensure InterviewSession handles its own initialization errors gracefully
        session_obj = InterviewSession(interview_id, resume_text, jd_text)
        init_state_info = session_obj.get_state() # Check state immediately after init

        if init_state_info["state"] == "ERROR":
            logger.error(f"[{interview_id}] Session initialization failed. Error: {init_state_info['error']}")
            # Don't store the failed session or set it in flask_session
            return jsonify({"error": f"Interview initialization failed: {init_state_info['error']}"}), 500

        store_session(interview_id, session_obj)
        flask_session['interview_id'] = interview_id # Store new ID in Flask session cookie
        logger.debug(f"Stored interview_id {interview_id} in Flask session.")

        elapsed_time = time.time() - start_time
        logger.info(f"[{interview_id}] Interview setup completed in {elapsed_time:.2f} seconds.")
        # Return 200 OK on successful initialization
        return jsonify({"interview_id": interview_id,"message": "Interview initialized successfully. Ready for greeting."}), 200

    except Exception as e:
        # Log the full traceback for unexpected errors
        logger.exception("Unexpected error in /start-interview")
        if isinstance(e, werkzeug.exceptions.RequestEntityTooLarge):
              return jsonify({"error": f"File upload failed. The resume file may be too large (limit: {app.config['MAX_CONTENT_LENGTH'] // 1024 // 1024} MB)."}), 413
        # Return a generic error message to the client
        return jsonify({"error": f"An unexpected server error occurred during setup."}), 500


# ... (other routes: /get-ai-message, /submit-response, /get-report - keep as is, CORS applies to them too) ...
@app.route('/get-ai-message', methods=['GET'])
def get_ai_message():
    interview_id = flask_session.get('interview_id')
    if not interview_id: return jsonify({"error": "No active interview session found. Please start a new interview."}), 400
    logger.info(f"[{interview_id}] Received request for next AI message.")
    session_obj = get_session(interview_id)
    if not session_obj:
        flask_session.pop('interview_id', None) # Clean up stale session ID
        return jsonify({"error": "Interview session not found or expired. Please start again."}), 404
    try:
        state_info = session_obj.get_state()
        current_state = state_info["state"]
        logger.debug(f"[{interview_id}] Current session state: {current_state}")

        ai_message = None
        response_status = 200

        if current_state == "READY": ai_message = session_obj.get_greeting()
        elif current_state == "ASKING": ai_message = session_obj.get_next_ai_turn()
        elif current_state == "AWAITING_RESPONSE": ai_message = session_obj.last_ai_message # Re-send last question if requested again
        elif current_state in ["FINISHED", "EVALUATING"]: ai_message = session_obj.last_ai_message or "The interview has concluded."
        elif current_state == "ERROR":
             ai_message = state_info.get("error", "An unspecified error occurred in the interview session.") # Use .get for safety
             response_status = 500
        else:
            # Should not happen in normal flow, but handle gracefully
             ai_message = "Interview is currently processing, please wait..."
             response_status = 202 # Accepted, processing

        # Re-check state in case generating the message caused an error
        final_state_info = session_obj.get_state()
        if final_state_info["state"] == "ERROR":
            ai_message = final_state_info.get("error", "An error occurred while generating the AI message.")
            response_status = 500

        logger.info(f"[{interview_id}] Sending AI message. Final state: {final_state_info['state']}")
        return jsonify({"ai_message": ai_message, "status": final_state_info["state"]}), response_status

    except Exception as e:
        logger.exception(f"[{interview_id}] Unexpected error in /get-ai-message")
        # Try to set error state in session if possible
        if session_obj and session_obj.get_state()["state"] != "ERROR":
             session_obj._set_error_state(f"Server error during AI turn generation: {e}")
        return jsonify({"error": f"An unexpected server error occurred.", "status": "ERROR"}), 500

@app.route('/submit-response', methods=['POST'])
def submit_response():
    interview_id = flask_session.get('interview_id')
    if not interview_id: return jsonify({"error": "No active interview session found. Please start a new interview."}), 400
    logger.info(f"[{interview_id}] Received request to submit response.")
    session_obj = get_session(interview_id)
    if not session_obj:
        flask_session.pop('interview_id', None)
        return jsonify({"error": "Interview session not found or expired. Please start again."}), 404

    current_state = session_obj.get_state()["state"]
    if current_state != "AWAITING_RESPONSE":
         logger.warning(f"[{interview_id}] Response submitted while not awaiting. State: {current_state}")
         # Send back the current AI message/state instead of a hard error? Or a specific message.
         return jsonify({"error": f"Cannot submit response now, the system is not awaiting your answer (state: {current_state})."}), 409 # Conflict

    if 'audio_data' not in request.files: return jsonify({"error": "Audio data blob is required."}), 400
    audio_file = request.files['audio_data']
    if not audio_file or audio_file.filename == '': return jsonify({"error": "No audio file selected/uploaded."}), 400

    # Determine file extension more robustly if needed (e.g., from mimetype if provided by browser)
    file_ext = ".wav" # Default if no extension found
    if '.' in audio_file.filename:
        ext_candidate = audio_file.filename.rsplit('.', 1)[1].lower()
        # Be specific about allowed extensions if audio processing depends on it
        if ext_candidate in ['wav', 'webm', 'ogg', 'mp3', 'flac', 'm4a']: # Add common types
             file_ext = f".{ext_candidate}"
        else:
             logger.warning(f"[{interview_id}] Received audio file with potentially unsupported extension: {ext_candidate}")
             # Decide if you want to reject or try processing anyway

    # Use a consistent turn number for naming
    qna_turn_number = session_obj.current_turn_number # Use the session's current turn tracker

    # Ensure filename is safe and unique enough
    filename = secure_filename(f"{interview_id}_turn_{qna_turn_number}_response{file_ext}")
    temp_audio_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)

    result = {"status": "error", "message": "Audio processing did not complete."}
    response_status = 500 # Default error status

    try:
        logger.debug(f"[{interview_id}] Saving temporary audio response for QnA turn {qna_turn_number} to: {temp_audio_path}")
        audio_file.save(temp_audio_path)
        audio_size = os.path.getsize(temp_audio_path)
        logger.info(f"[{interview_id}] Audio file saved successfully (Size: {audio_size} bytes).")

        if audio_size == 0:
             logger.warning(f"[{interview_id}] Received empty audio file.")
             raise ValueError("Received empty audio file.") # Treat as error

        logger.info(f"[{interview_id}] Processing candidate response via InterviewSession...")
        # Make sure process_candidate_response handles its own errors and returns structured result
        result = session_obj.process_candidate_response(temp_audio_path)
        logger.info(f"[{interview_id}] Response processing result: {result}")

        # Determine status based on session's report
        final_state_info = session_obj.get_state()
        if final_state_info["state"] == "ERROR":
             result = {"status": "error", "message": final_state_info.get("error", "Error processing response.")}
             response_status = 500
        elif result.get("status") == "success": # Check the direct result *and* session state
             response_status = 200 # OK
        else: # If result indicates failure but session isn't ERROR state yet
             response_status = 400 # Bad request (e.g., transcription failed but session is recoverable)

    except werkzeug.exceptions.RequestEntityTooLarge:
         logger.warning(f"[{interview_id}] Audio upload failed: File too large.")
         result = {"status": "error", "message": f"Audio file is too large (limit: {app.config['MAX_CONTENT_LENGTH'] // 1024 // 1024} MB)."}
         response_status = 413
    except ValueError as ve: # Catch specific errors like empty file
         logger.error(f"[{interview_id}] Value error processing audio: {ve}")
         result = {"status": "error", "message": str(ve)}
         response_status = 400 # Bad request
    except Exception as e:
        logger.exception(f"[{interview_id}] Error saving or processing audio response")
        result = {"status": "error", "message": f"Server error processing audio."}
        # Ensure session state reflects error if possible
        if session_obj and session_obj.get_state()["state"] != "ERROR":
             session_obj._set_error_state(f"Server error processing audio: {e}")
        response_status = 500
    finally:
        # Always try to remove the temporary file
        if os.path.exists(temp_audio_path):
            try: os.remove(temp_audio_path)
            except OSError as e: logger.warning(f"[{interview_id}] Could not remove temp audio file {temp_audio_path}: {e}")

    # Return based on the determined status
    if response_status == 200:
        return jsonify({"status": "OK", "message": "Response received and processed."}), 200
    else:
        # Provide the specific error message from the result dict
        return jsonify({"error": result.get("message", "Failed to process response.")}), response_status


@app.route('/get-report', methods=['GET'])
def get_report():
    interview_id = flask_session.get('interview_id')
    if not interview_id: return jsonify({"error": "No active interview session found to generate report."}), 400
    logger.info(f"[{interview_id}] Received request to get report.")
    session_obj = get_session(interview_id)
    if not session_obj:
        flask_session.pop('interview_id', None)
        return jsonify({"error": "Interview session not found or expired."}), 404

    current_state = session_obj.get_state()["state"]
    # Allow report generation even if state is ERROR, as evaluation might have partially completed
    if current_state not in ["FINISHED", "EVALUATING", "ERROR"]:
         logger.warning(f"[{interview_id}] Report requested before interview finished. State: {current_state}")
         return jsonify({"error": f"Cannot generate report yet. Interview status: {current_state}."}), 409 # Conflict

    # Ensure final evaluation runs if needed and possible
    if current_state != "ERROR" and not session_obj.evaluation_complete:
         logger.info(f"[{interview_id}] Evaluation not complete. Performing final evaluation before report generation.")
         try:
              eval_success = session_obj.perform_final_evaluation()
              if not eval_success:
                   logger.error(f"[{interview_id}] Final evaluation failed. Report may be incomplete.")
                   # Optionally, return an error here if evaluation is critical for the report
              else:
                   logger.info(f"[{interview_id}] Final evaluation completed.")
         except Exception as eval_err:
              logger.exception(f"[{interview_id}] Error during final evaluation")
              # Decide if report generation should proceed or fail
              return jsonify({"error": f"Error during final report evaluation: {eval_err}"}), 500

    logger.info(f"[{interview_id}] Attempting to generate report...")
    try:
        # generate_report should handle its internal errors and return None on failure
        report_path = session_obj.generate_report()

        if report_path and os.path.exists(report_path):
            logger.info(f"[{interview_id}] Sending report file: {report_path}")
            # Ensure correct mimetype for PDF
            return send_file(report_path, as_attachment=True, download_name=os.path.basename(report_path), mimetype='application/pdf')
        else:
            logger.error(f"[{interview_id}] Report generation failed or file not found. Report path returned: {report_path}")
            # Get specific error from session if available
            error_msg = session_obj.get_state().get("error", "Failed to generate or find the interview report.")
            return jsonify({"error": error_msg}), 500

    except Exception as report_err:
        logger.exception(f"[{interview_id}] Unexpected error during report generation or sending")
        return jsonify({"error": f"An unexpected server error occurred while generating the report: {report_err}"}), 500

print("--- app.py: Routes defined ---") # DEBUG

# --- Error Handling ---
@app.errorhandler(404)
def not_found_error(error):
     logger.warning(f"404 Not Found: {request.url}")
     # Return JSON for API consistency
     return jsonify({"error": "Not Found", "message": "The requested URL was not found on the server."}), 404

@app.errorhandler(400)
def bad_request_error(error):
     # Get description from the error object if available
     error_desc = getattr(error, 'description', 'No description provided.')
     logger.warning(f"400 Bad Request: {request.url} - {error_desc}")
     return jsonify({"error": "Bad Request", "message": error_desc}), 400

@app.errorhandler(405) # Method Not Allowed
def method_not_allowed_error(error):
     logger.warning(f"405 Method Not Allowed: {request.method} for {request.url}")
     return jsonify({"error": "Method Not Allowed", "message": "The method is not allowed for the requested URL."}), 405

@app.errorhandler(413)
def request_entity_too_large_error(error):
     logger.warning(f"413 Request Entity Too Large: {request.url} - Content Length: {request.content_length}")
     limit_mb = app.config.get('MAX_CONTENT_LENGTH', 16*1024*1024) // 1024 // 1024 # Default 16MB if not set
     return jsonify({"error": "Request Entity Too Large", "message": f"The uploaded file exceeds the maximum allowed size of {limit_mb} MB."}), 413

# Generic Exception Handler - Keep this last
@app.errorhandler(Exception)
def handle_exception(e):
    # Log the full traceback for internal debugging
    error_id = uuid.uuid4()
    tb_str = traceback.format_exc()
    logger.error(f"Unhandled Exception (ID: {error_id}): {e}\n--- Traceback ---\n{tb_str}--- End Traceback ---")

    # Specific handling for Werkzeug HTTP exceptions if needed, though specific handlers above are better
    if isinstance(e, werkzeug.exceptions.HTTPException):
         # Use the exception's code and default message if available
         response = e.get_response()
         return jsonify({"error": e.name, "message": e.description}), e.code

    # For all other non-HTTP exceptions, return a generic 500
    return jsonify({"error": "Internal Server Error",
                    "message": "An unexpected server error occurred. Please contact support or check server logs.",
                    "error_id": str(error_id)}), 500

print("--- app.py: Error handlers defined ---") # DEBUG


# Main execution block (if running directly, e.g., python app.py)
# Note: `flask run` command doesn't typically use this block directly,
# but it's good practice for potential direct execution scenarios.
if __name__ == '__main__':
    print("--- app.py: Running in __main__ block ---") # DEBUG
    # Check if initialization was successful before running
    if not initialization_complete:
         print("--- app.py: ERROR - Initialization did not complete successfully. Exiting. ---")
         sys.exit(1)

    # Get host and port from config or environment variables for flexibility
    host = os.environ.get('FLASK_RUN_HOST', '0.0.0.0') # Use 0.0.0.0 for external accessibility
    port = int(os.environ.get('FLASK_RUN_PORT', 5050))
    debug_mode = config.DEBUG # Get debug mode from config

    print(f"--- app.py: Starting Flask development server on {host}:{port} (Debug: {debug_mode}) ---") # DEBUG
    # Use app.run() only if running script directly, NOT with `flask run`
    # `flask run` handles its own server setup (like Gunicorn or Werkzeug's runner)
    # Generally, you'd use `flask run` for development.
    # app.run(host=host, port=port, debug=debug_mode)
    print("--- app.py: Script execution finished (Note: Use 'flask run' to start the server) ---") # DEBUG

    # flask run --host=0.0.0.0 --port=5050

# auth.py
from flask import Blueprint, render_template, redirect, url_for, flash, request
from flask_login import login_user, logout_user, login_required, current_user
# Import db and models, bcrypt from models.py
from models import db, User, PasswordReset, bcrypt
from itsdangerous import URLSafeTimedSerializer, SignatureExpired, BadTimeSignature
from email_validator import validate_email, EmailNotValidError
import datetime

# Import mail object and serializers from app context
from flask import current_app as app # Use app context

# Define Blueprint
auth_bp = Blueprint('auth', __name__, template_folder='templates/auth')

# --- Helper Functions (send_email, generate/confirm email token) ---
# Keep these as they were in the previous version

def send_email(to, subject, template):
    try:
        # Ensure mail is initialized in the app context
        mail = app.extensions.get('mail')
        if not mail:
            app.logger.error("Flask-Mail extension not found or initialized.")
            return False

        msg = Message(
            subject,
            recipients=[to],
            html=template,
            sender=app.config['MAIL_DEFAULT_SENDER']
        )
        mail.send(msg)
        app.logger.info(f"Email sent successfully to {to} with subject '{subject}'")
        return True
    except Exception as e:
        app.logger.error(f"Error sending email to {to}: {e}", exc_info=True)
        return False

def generate_confirmation_token(email):
    serializer = URLSafeTimedSerializer(app.config['SECRET_KEY'])
    return serializer.dumps(email, salt=app.config['EMAIL_CONFIRMATION_SALT'])

def confirm_token(token, expiration=None):
    serializer = URLSafeTimedSerializer(app.config['SECRET_KEY'])
    expiration = expiration or app.config['EMAIL_TOKEN_EXPIRATION']
    try:
        email = serializer.loads(
            token,
            salt=app.config['EMAIL_CONFIRMATION_SALT'],
            max_age=expiration
        )
        return email
    except (SignatureExpired, BadTimeSignature):
        return False
    except Exception as e:
        app.logger.error(f"Error confirming token: {e}", exc_info=True)
        return False

# --- Password Reset Token Helpers (Modified) ---

# Helper to generate password reset token (using itsdangerous) - returns the raw token string
def generate_password_reset_itsdangerous_token(user_id):
     serializer = URLSafeTimedSerializer(app.config['SECRET_KEY'])
     return serializer.dumps(str(user_id), salt=app.config['SECURITY_PASSWORD_SALT'])

# Helper to verify password reset token (using itsdangerous) - checks expiry and signature
def verify_password_reset_itsdangerous_token(token, expiration=None):
     serializer = URLSafeTimedSerializer(app.config['SECRET_KEY'])
     expiration = expiration or app.config['PASSWORD_RESET_TOKEN_EXPIRATION']
     try:
         user_id = serializer.loads(
             token,
             salt=app.config['SECURITY_PASSWORD_SALT'],
             max_age=expiration
         )
         return int(user_id) # Return user ID if valid and not expired
     except (SignatureExpired, BadTimeSignature):
         app.logger.debug(f"Password reset token expired or invalid signature: {token[:10]}...")
         return None # Indicates expired or invalid signature
     except Exception as e:
         app.logger.error(f"Error verifying password reset token: {e}", exc_info=True)
         return None


# --- Routes (Register, Confirm, Login, Logout - Keep as before) ---
@auth_bp.route('/register', methods=['GET', 'POST'])
def register():
    if current_user.is_authenticated:
        return redirect(url_for('interview_page'))

    if request.method == 'POST':
        username = request.form.get('username')
        email = request.form.get('email')
        password = request.form.get('password')
        confirm_password = request.form.get('confirm_password')
        error = None

        if not username or not email or not password or not confirm_password:
            error = 'All fields are required.'
        elif password != confirm_password:
            error = 'Passwords do not match.'
        else:
            try:
                valid = validate_email(email)
                email = valid.email # Normalized email
            except EmailNotValidError as e:
                error = str(e)

        if error is None:
            existing_user_email = User.query.filter_by(email=email).first()
            existing_user_username = User.query.filter_by(username=username).first()

            if existing_user_email:
                error = f'Email {email} is already registered.'
            elif existing_user_username:
                error = f'Username {username} is already taken.'

        if error is None:
            try:
                new_user = User(username=username, email=email, password=password, is_confirmed=False)
                db.session.add(new_user)
                db.session.commit()
                app.logger.info(f"User {username} ({email}) created, awaiting confirmation.")

                # Send confirmation email
                token = generate_confirmation_token(email)
                confirm_url = url_for('auth.confirm_email', token=token, _external=True)
                html = render_template('email/confirm_email.html', confirm_url=confirm_url) # Create this template
                subject = "Please confirm your email address"

                if send_email(new_user.email, subject, html):
                    flash('A confirmation email has been sent to your email address. Please check your inbox (and spam folder).', 'success')
                    return redirect(url_for('auth.login'))
                else:
                    # Rollback user creation if email sending fails
                    db.session.rollback()
                    app.logger.error(f"Email sending failed for {email} during registration. User creation rolled back.")
                    flash('Could not send confirmation email. Please try registering again later or contact support.', 'danger')
                    error = "Registration failed due to email error." # Update error state

            except Exception as e:
                 db.session.rollback()
                 app.logger.error(f"Error during registration for {username}: {e}", exc_info=True)
                 flash(f'An error occurred during registration. Please try again.', 'danger')
                 error = "Registration failed due to a server error."

        if error:
            flash(error, 'danger')

    return render_template('register.html')

@auth_bp.route('/confirm/<token>')
def confirm_email(token):
    try:
        email = confirm_token(token)
    except: # Catches general errors from confirm_token logic
        email = None # Ensure email is None if any error occurs

    if not email:
         flash('The confirmation link is invalid or has expired.', 'danger')
         return redirect(url_for('auth.login'))

    user = User.query.filter_by(email=email).first()

    # Handle case where user might not exist (though unlikely if token was valid)
    if not user:
         flash('User associated with this confirmation link not found.', 'danger')
         return redirect(url_for('auth.register')) # Or login page

    if user.is_confirmed:
        flash('Account already confirmed. Please login.', 'info')
    else:
        try:
            user.is_confirmed = True
            # user.confirmed_on = datetime.datetime.now() # Optional: track confirmation time
            db.session.commit()
            flash('You have confirmed your account. Thanks!', 'success')
            app.logger.info(f"User {user.username} confirmed email successfully.")
            login_user(user) # Log in automatically
            return redirect(url_for('interview_page')) # Redirect to main app page
        except Exception as e:
             db.session.rollback()
             app.logger.error(f"Error updating user confirmation status for {email}: {e}", exc_info=True)
             flash('An error occurred during account confirmation. Please try again later.', 'danger')

    return redirect(url_for('auth.login'))

@auth_bp.route('/login', methods=['GET', 'POST'])
def login():
    if current_user.is_authenticated:
        return redirect(url_for('interview_page'))

    if request.method == 'POST':
        email = request.form.get('email')
        password = request.form.get('password')
        remember = True if request.form.get('remember') else False
        error = None

        if not email or not password:
            error = 'Email and password are required.'
        else:
            user = User.query.filter_by(email=email).first()

            if not user:
                error = 'Email address not found.'
            elif user.is_locked:
                error = 'Your account is locked due to too many failed login attempts. Please reset your password.'
            elif not user.is_confirmed:
                 # Optional: Re-send confirmation link?
                 # confirm_token = generate_confirmation_token(user.email)
                 # confirm_url = url_for('auth.confirm_email', token=confirm_token, _external=True)
                 # flash(f'Your account is not confirmed. Please check your email for the confirmation link. <a href="{confirm_url}">Resend?</a>', 'warning') # Example resend
                 error = 'Your account is not confirmed. Please check your email for the confirmation link.'
            elif not user.check_password(password):
                try:
                    user.failed_attempts = (user.failed_attempts or 0) + 1
                    if user.failed_attempts >= app.config['FAILED_LOGIN_ATTEMPTS_LOCKOUT']:
                        user.is_locked = True
                        error = 'Your account has been locked due to too many failed login attempts. Please reset your password.'
                        app.logger.warning(f"Account locked for user {user.email}.")
                    else:
                         error = 'Incorrect password. Please try again.'
                         remaining = app.config['FAILED_LOGIN_ATTEMPTS_LOCKOUT'] - user.failed_attempts
                         flash(f'{remaining} login attempts remaining before account lock.', 'warning')
                    db.session.commit()
                except Exception as e:
                    db.session.rollback()
                    app.logger.error(f"Error updating failed attempts for {email}: {e}", exc_info=True)
                    error = "An error occurred during login. Please try again."
            else:
                # Password is correct, login successful
                try:
                    user.failed_attempts = 0
                    user.is_locked = False # Ensure unlocked on successful login
                    db.session.commit()
                    login_user(user, remember=remember)
                    app.logger.info(f"User {user.username} logged in successfully.")
                    next_page = request.args.get('next')
                    return redirect(next_page or url_for('interview_page'))
                except Exception as e:
                     db.session.rollback()
                     app.logger.error(f"Error updating user status on login for {email}: {e}", exc_info=True)
                     error = "An error occurred finalizing login. Please try again."

        if error:
            flash(error, 'danger')

    return render_template('login.html')

@auth_bp.route('/logout')
@login_required
def logout():
    user_id = current_user.id
    username = current_user.username
    logout_user()
    flash('You have been logged out.', 'success')
    app.logger.info(f"User {username} (ID: {user_id}) logged out.")
    return redirect(url_for('auth.login'))


# --- Password Reset Request Route (Modified) ---
@auth_bp.route('/reset_password_request', methods=['GET', 'POST'])
def reset_password_request():
    if current_user.is_authenticated:
        return redirect(url_for('interview_page'))

    if request.method == 'POST':
        email = request.form.get('email')
        user = User.query.filter_by(email=email).first()

        if user:
             if not user.is_confirmed:
                  flash('Your account is not confirmed. Please confirm your email first.', 'warning')
             elif user.is_locked:
                  # Allow locked users to reset password
                   pass # Continue with reset process even if locked
             # Check if an active (non-expired) reset request already exists
             # existing_reset = PasswordReset.query.filter_by(user_id=user.id)\
             #                                    .filter(PasswordReset.expires_at > datetime.datetime.utcnow())\
             #                                    .first()
             # if existing_reset:
             #     flash('A password reset link has already been sent recently. Please check your email or wait.', 'info')
             #     return redirect(url_for('auth.login'))

             try:
                 # Generate the itsdangerous token (contains user ID and expiry)
                 raw_token = generate_password_reset_itsdangerous_token(user.id)
                 # Calculate expiry time based on config
                 expiry_duration = datetime.timedelta(seconds=app.config['PASSWORD_RESET_TOKEN_EXPIRATION'])
                 expires_at = datetime.datetime.utcnow() + expiry_duration

                 # Create and save the PasswordReset record with the *hashed* token
                 # Note: This will hash the raw_token generated above
                 new_reset_request = PasswordReset(user_id=user.id, token=raw_token, expires_at=expires_at)

                 # Optional: Delete any previous reset records for this user
                 PasswordReset.query.filter_by(user_id=user.id).delete()

                 db.session.add(new_reset_request)
                 db.session.commit()

                 # Send email with the raw (itsdangerous) token
                 reset_url = url_for('auth.reset_password', token=raw_token, _external=True)
                 html = render_template('email/reset_password.html', reset_url=reset_url) # Template needs only reset_url
                 subject = "Password Reset Request"

                 if send_email(user.email, subject, html):
                     flash('A password reset link has been sent to your email address.', 'success')
                     app.logger.info(f"Password reset email sent for user {user.email}")
                 else:
                      db.session.rollback() # Rollback the PasswordReset record if email fails
                      app.logger.error(f"Failed to send password reset email to {user.email}. Reset record rolled back.")
                      flash('Could not send password reset email. Please try again later or contact support.', 'danger')

             except Exception as e:
                  db.session.rollback()
                  app.logger.error(f"Error generating/sending password reset for {email}: {e}", exc_info=True)
                  flash('An error occurred while processing your request. Please try again.', 'danger')
        else:
             # Don't reveal if the email exists or not for security
             flash('If an account with that email exists and is confirmed, a password reset link has been sent.', 'info')
             app.logger.info(f"Password reset requested for non-existent or unconfirmed email: {email}")

        # Redirect to login even after processing to prevent POST resubmission on refresh
        return redirect(url_for('auth.login'))

    return render_template('reset_password_request.html')


# --- Password Reset Route (with token) (Modified) ---
@auth_bp.route('/reset_password/<token>', methods=['GET', 'POST'])
def reset_password(token):
    if current_user.is_authenticated:
        return redirect(url_for('interview_page'))

    # 1. Verify token using itsdangerous (checks expiry and signature)
    user_id = verify_password_reset_itsdangerous_token(token)
    if user_id is None:
         flash('The password reset link is invalid or has expired.', 'danger')
         return redirect(url_for('auth.login'))

    # 2. Find the corresponding PasswordReset record in the DB using the user_id
    #    Order by created_at descending to get the latest one if multiple somehow exist (shouldn't happen with delete logic)
    reset_record = PasswordReset.query.filter_by(user_id=user_id)\
                                      .order_by(PasswordReset.created_at.desc())\
                                      .first()

    # 3. Check if a record exists and if the *provided token's hash matches the stored hash*
    if not reset_record or not reset_record.check_token(token):
        # If no record, or if the hash doesn't match, the token is invalid or already used
        flash('The password reset link is invalid or has already been used.', 'danger')
        app.logger.warning(f"Invalid or used password reset token presented for user_id {user_id}. Token: {token[:10]}...")
        return redirect(url_for('auth.login'))

    # If we reach here, the token is valid (itsdangerous checks passed) AND
    # it matches the hash stored in the database (bcrypt check passed)

    user = User.query.get(user_id)
    if not user:
         # Should not happen if user_id was valid, but safety check
         flash('User not found for this reset link.', 'danger')
         # Optionally delete the now invalid reset_record here
         if reset_record:
             db.session.delete(reset_record)
             db.session.commit()
         return redirect(url_for('auth.login'))

    # --- Handle POST request to set new password ---
    if request.method == 'POST':
        password = request.form.get('password')
        confirm_password = request.form.get('confirm_password')
        error = None

        if not password or not confirm_password:
            error = 'Both password fields are required.'
        elif len(password) < 8: # Add minimum password length check
             error = 'Password must be at least 8 characters long.'
        elif password != confirm_password:
            error = 'Passwords do not match.'

        if error is None:
            try:
                # Set the new password (hashes it)
                user.set_password(password)
                user.failed_attempts = 0 # Reset failed attempts
                user.is_locked = False # Unlock account
                # Invalidate the reset record by deleting it
                db.session.delete(reset_record)
                db.session.commit()

                flash('Your password has been successfully reset. Please login with your new password.', 'success')
                app.logger.info(f"Password reset successfully for user {user.email}")
                return redirect(url_for('auth.login'))
            except Exception as e:
                 db.session.rollback()
                 app.logger.error(f"Error resetting password or deleting reset record for user {user.email}: {e}", exc_info=True)
                 flash('An error occurred while resetting your password. Please try again.', 'danger')
                 error = "Password reset failed due to a server error."

        # If there was an error during POST
        if error:
            flash(error, 'danger')

    # For GET request or POST with errors, show the form
    return render_template('reset_password.html', token=token)

import os
from dotenv import load_dotenv
import warnings
import logging

load_dotenv() # Loads variables from .env file into environment
logger = logging.getLogger(__name__) # Logger for config issues

# --- General ---
NUM_QUESTIONS = 6
DEBUG = os.environ.get("FLASK_DEBUG", "False").lower() == "true"

# --- File Paths ---
UPLOAD_FOLDER = os.path.abspath(os.getenv("UPLOAD_FOLDER", "temp_uploads"))
REPORT_FOLDER = os.path.abspath(os.getenv("REPORT_FOLDER", "reports"))
REPORT_FILENAME_TEMPLATE = "{interview_id}_interview_report.pdf"

# --- SQLAlchemy Database (Users, Reports, Auth Data) ---
SQLALCHEMY_DB_NAME = os.getenv("SQLALCHEMY_DB_NAME", "users") # Database for Flask App data
SQLALCHEMY_DB_USER = os.getenv("SQLALCHEMY_DB_USER") # *** SET IN .env ***
SQLALCHEMY_DB_PASSWORD = os.getenv("SQLALCHEMY_DB_PASSWORD") # *** SET IN .env ***
SQLALCHEMY_DB_HOST = os.getenv("SQLALCHEMY_DB_HOST", "localhost")
SQLALCHEMY_DB_PORT = os.getenv("SQLALCHEMY_DB_PORT", "5432")

# --- SQLAlchemy Configuration ---
# Check if credentials are provided before constructing the URI
if not all([SQLALCHEMY_DB_USER, SQLALCHEMY_DB_PASSWORD]):
    logger.warning("SQLAlchemy DB User or Password not set in environment. Database features will fail.")
    SQLALCHEMY_DATABASE_URI = None # Set to None if incomplete
else:
    SQLALCHEMY_DATABASE_URI = f"postgresql://{SQLALCHEMY_DB_USER}:{SQLALCHEMY_DB_PASSWORD}@{SQLALCHEMY_DB_HOST}:{SQLALCHEMY_DB_PORT}/{SQLALCHEMY_DB_NAME}"
SQLALCHEMY_TRACK_MODIFICATIONS = False # Recommended setting

# --- RAG Database (Knowledge Base - Accessed via psycopg2) ---
RAG_ENABLED = os.environ.get("RAG_ENABLED", "True").lower() == "true" # Allow disabling RAG easily
RAG_DB_NAME = os.getenv("RAG_DB_NAME", "knowledgebase") # Separate DB for RAG content
RAG_DB_USER = os.getenv("RAG_DB_USER") # *** SET IN .env (can be same as SQLALCHEMY user or different) ***
RAG_DB_PASSWORD = os.getenv("RAG_DB_PASSWORD") # *** SET IN .env ***
RAG_DB_HOST = os.getenv("RAG_DB_HOST", "localhost")
RAG_DB_PORT = os.getenv("RAG_DB_PORT", "5432")

# --- RAG Configuration ---
EMBEDDING_MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'
MAX_CONTEXT_LENGTH = 10000
RETRIEVAL_TOP_K = int(os.getenv("RETRIEVAL_TOP_K", "6")) # Number of docs to retrieve
RETRIEVAL_SIMILARITY_THRESHOLD = float(os.getenv("RETRIEVAL_SIMILARITY_THRESHOLD", "0.58"))

# Disable RAG retrieval if RAG_ENABLED is False
if not RAG_ENABLED:
    RETRIEVAL_TOP_K = 0
    logger.info("RAG feature is disabled via RAG_ENABLED setting.")

# --- Google Cloud ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CLOUD_PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT_ID") # Needed for STT

# --- ElevenLabs (Relevant if NeuroSync Player uses it) ---
# ... (keep as before) ...
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
ELEVENLABS_VOICE_ID = os.getenv("ELEVENLABS_VOICE_ID", "21m00Tcm4TlvDq8ikWAM")

# --- TTS Provider Selection (Relevant for NeuroSync Player configuration hint) ---
# ... (keep as before) ...
TTS_PROVIDER_HINT = os.getenv("TTS_PROVIDER", "google").lower()

# --- LLM Configuration (Google Gemini) ---
# ... (keep as before) ...
INTERVIEWER_LLM_MODEL_NAME = "gemini-1.5-flash-latest"
EVALUATOR_LLM_MODEL_NAME = "gemini-1.5-pro-latest"

# --- LLM Generation Parameters ---
# ... (keep as before) ...
INTERVIEWER_MAX_TOKENS = 500
INTERVIEWER_TEMPERATURE = 0.65
EVALUATOR_MAX_TOKENS = 700
EVALUATOR_TEMPERATURE = 0.5

# --- Prompting Constants ---
# ... (keep as before) ...
MAX_SUMMARY_LENGTH = 1200
MAX_PROJECT_SUMMARY_LENGTH = 800
MAX_TOTAL_PROMPT_CHARS = 100000

# --- Simulation Details (Defaults for report/prompts if needed) ---
# ... (keep as before) ...
CANDIDATE_NAME = "Candidate"
INTERVIEWER_AI_NAME = "Alexi"
COMPANY_NAME = "SecureData Financial Corp."

# --- Audio Configuration ---
# ... (keep as before) ...
DEFAULT_AUDIO_SAMPLERATE = 16000
GOOGLE_STT_LANGUAGE_CODE = "en-US"

# --- Emotion Analysis API ---
EMOTION_API_ENDPOINT = os.environ.get("EMOTION_API_ENDPOINT", "http://127.0.0.1:5003/analyze")

# --- NeuroSync Player Network Configuration ---
NEUROSYNC_PLAYER_HOST = os.environ.get("NEUROSYNC_PLAYER_HOST", '127.0.0.1')
NEUROSYNC_PLAYER_PORT = int(os.environ.get("NEUROSYNC_PLAYER_PORT", 5678)) # Ensure it's an int

# --- Logging ---
LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()

# --- Flask Specific ---
SECRET_KEY = os.getenv("FLASK_SECRET_KEY", "a_very_insecure_default_dev_key_change_me")
if SECRET_KEY == "a_very_insecure_default_dev_key_change_me":
    logger.warning("SECURITY WARNING: Using default Flask SECRET_KEY. Set a strong FLASK_SECRET_KEY environment variable for production.")

# --- Flask-Mail Configuration ---
# ... (keep as before) ...
MAIL_SERVER = os.getenv('MAIL_SERVER', 'smtp.googlemail.com')
MAIL_PORT = int(os.getenv('MAIL_PORT', '587'))
MAIL_USE_TLS = os.getenv('MAIL_USE_TLS', 'true').lower() == 'true'
MAIL_USE_SSL = os.getenv('MAIL_USE_SSL', 'false').lower() == 'true'
MAIL_USERNAME = os.getenv('MAIL_USERNAME')
MAIL_PASSWORD = os.getenv('MAIL_PASSWORD')
MAIL_DEFAULT_SENDER = os.getenv('MAIL_DEFAULT_SENDER', MAIL_USERNAME)

# --- Token Generation (itsdangerous) ---
# ... (keep as before) ...
SECURITY_PASSWORD_SALT = os.getenv('SECURITY_PASSWORD_SALT', 'password_salt_change_me')
EMAIL_CONFIRMATION_SALT = os.getenv('EMAIL_CONFIRMATION_SALT', 'email_salt_change_me')
EMAIL_TOKEN_EXPIRATION = 3600
PASSWORD_RESET_TOKEN_EXPIRATION = 1800

# --- Login Configuration ---
FAILED_LOGIN_ATTEMPTS_LOCKOUT = 5


# --- Final Checks ---
# Check essential configurations needed by the Flask app
if not GOOGLE_API_KEY:
    warnings.warn("Configuration Warning: GOOGLE_API_KEY is not set. LLM interactions will fail.")
    logger.warning("GOOGLE_API_KEY is not set.")

if not GOOGLE_CLOUD_PROJECT_ID:
     warnings.warn("Configuration Warning: GOOGLE_CLOUD_PROJECT_ID is not set. Google STT will fail.")
     logger.warning("GOOGLE_CLOUD_PROJECT_ID is not set.")

if not SQLALCHEMY_DATABASE_URI:
    warnings.warn("Configuration Warning: SQLALCHEMY Database URI is not configured (check SQLALCHEMY_DB_USER/PASSWORD). App DB features will fail.")
    logger.warning("SQLALCHEMY_DATABASE_URI is not configured.")

# Check Mail config if MAIL_USERNAME is set (indicates intent to use mail)
if MAIL_USERNAME and not MAIL_PASSWORD:
     warnings.warn("Configuration Warning: MAIL_USERNAME is set, but MAIL_PASSWORD is missing. Email sending will likely fail.")
     logger.warning("MAIL_PASSWORD is not set.")

# Check RAG DB config only if RAG is intended to be active
if RAG_ENABLED and RETRIEVAL_TOP_K > 0:
    if not all([RAG_DB_NAME, RAG_DB_USER, RAG_DB_PASSWORD, RAG_DB_HOST]):
         warnings.warn("Configuration Warning: RAG is enabled, but RAG database connection details (RAG_DB_NAME, RAG_DB_USER, RAG_DB_PASSWORD, RAG_DB_HOST) are incomplete. RAG retrieval will fail.")
         logger.warning("RAG DB connection details may be incomplete.")
         # Optionally force disable RAG if config is bad
         # RETRIEVAL_TOP_K = 0
else:
     logger.info("RAG retrieval is disabled (RAG_ENABLED=False or RETRIEVAL_TOP_K=0).")


logger.info("Configuration loaded.")
if SQLALCHEMY_DATABASE_URI:
    logger.info(f"SQLAlchemy DB URI: postgresql://{SQLALCHEMY_DB_USER}:***@{SQLALCHEMY_DB_HOST}:{SQLALCHEMY_DB_PORT}/{SQLALCHEMY_DB_NAME}")
else:
    logger.warning("SQLAlchemy DB URI: Not configured.")
if RAG_ENABLED and RETRIEVAL_TOP_K > 0:
    logger.info(f"RAG DB Target: postgresql://{RAG_DB_USER}:***@{RAG_DB_HOST}:{RAG_DB_PORT}/{RAG_DB_NAME}")
logger.info(f"Mail Server: {MAIL_SERVER}:{MAIL_PORT} (Username: {MAIL_USERNAME})")
logger.info(f"Upload folder: {UPLOAD_FOLDER}")
logger.info(f"Report folder: {REPORT_FOLDER}")
logger.info(f"Emotion API endpoint: {EMOTION_API_ENDPOINT}")
logger.info(f"NeuroSync Player: {NEUROSYNC_PLAYER_HOST}:{NEUROSYNC_PLAYER_PORT}")

